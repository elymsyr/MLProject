{
    "name": "root",
    "gauges": {
        "AgentBehavior.Policy.Entropy.mean": {
            "value": 1.4575210809707642,
            "min": 1.4474471807479858,
            "max": 1.4575210809707642,
            "count": 25
        },
        "AgentBehavior.Policy.Entropy.sum": {
            "value": 29199.9765625,
            "min": 27995.076171875,
            "max": 29380.25390625,
            "count": 25
        },
        "AgentBehavior.Environment.EpisodeLength.mean": {
            "value": 177.33913043478262,
            "min": 177.33913043478262,
            "max": 194.16,
            "count": 25
        },
        "AgentBehavior.Environment.EpisodeLength.sum": {
            "value": 20394.0,
            "min": 18210.0,
            "max": 20394.0,
            "count": 25
        },
        "AgentBehavior.Step.mean": {
            "value": 1519964.0,
            "min": 1039990.0,
            "max": 1519964.0,
            "count": 25
        },
        "AgentBehavior.Step.sum": {
            "value": 1519964.0,
            "min": 1039990.0,
            "max": 1519964.0,
            "count": 25
        },
        "AgentBehavior.Policy.ExtrinsicValueEstimate.mean": {
            "value": -2.005098581314087,
            "min": -2.502894639968872,
            "max": -1.4852776527404785,
            "count": 25
        },
        "AgentBehavior.Policy.ExtrinsicValueEstimate.sum": {
            "value": -812.06494140625,
            "min": -996.1520385742188,
            "max": -598.56689453125,
            "count": 25
        },
        "AgentBehavior.Environment.CumulativeReward.mean": {
            "value": -9.549386190266713,
            "min": -10.662077651807556,
            "max": -8.893783382898956,
            "count": 25
        },
        "AgentBehavior.Environment.CumulativeReward.sum": {
            "value": -1098.179411880672,
            "min": -1151.504386395216,
            "max": -921.3343695476651,
            "count": 25
        },
        "AgentBehavior.Policy.ExtrinsicReward.mean": {
            "value": -9.549386190266713,
            "min": -10.662077651807556,
            "max": -8.893783382898956,
            "count": 25
        },
        "AgentBehavior.Policy.ExtrinsicReward.sum": {
            "value": -1098.179411880672,
            "min": -1151.504386395216,
            "max": -921.3343695476651,
            "count": 25
        },
        "AgentBehavior.Losses.PolicyLoss.mean": {
            "value": 0.026136934636936834,
            "min": 0.019825654658294903,
            "max": 0.02683260895676843,
            "count": 25
        },
        "AgentBehavior.Losses.PolicyLoss.sum": {
            "value": 0.05227386927387367,
            "min": 0.022919098087974512,
            "max": 0.05366521791353686,
            "count": 25
        },
        "AgentBehavior.Losses.ValueLoss.mean": {
            "value": 1.6872037172317504,
            "min": 1.5204419404268266,
            "max": 2.1331582705179852,
            "count": 25
        },
        "AgentBehavior.Losses.ValueLoss.sum": {
            "value": 3.374407434463501,
            "min": 1.745845341682434,
            "max": 4.2663165410359705,
            "count": 25
        },
        "AgentBehavior.Policy.LearningRate.mean": {
            "value": 0.00025473049508984,
            "min": 0.00025473049508984,
            "max": 0.0002690639203120299,
            "count": 25
        },
        "AgentBehavior.Policy.LearningRate.sum": {
            "value": 0.00050946099017968,
            "min": 0.0002588921837026099,
            "max": 0.0005371946609351198,
            "count": 25
        },
        "AgentBehavior.Policy.Epsilon.mean": {
            "value": 0.18491016,
            "min": 0.18491016,
            "max": 0.18968796999999993,
            "count": 25
        },
        "AgentBehavior.Policy.Epsilon.sum": {
            "value": 0.36982032,
            "min": 0.18629739,
            "max": 0.37906487999999994,
            "count": 25
        },
        "AgentBehavior.Policy.Beta.mean": {
            "value": 0.004247016984,
            "min": 0.004247016984,
            "max": 0.004485429703000001,
            "count": 25
        },
        "AgentBehavior.Policy.Beta.sum": {
            "value": 0.008494033968,
            "min": 0.004316239761,
            "max": 0.008955337511999999,
            "count": 25
        },
        "AgentBehavior.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 25
        },
        "AgentBehavior.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 25
        }
    },
    "metadata": {
        "timer_format_version": "0.1.0",
        "start_time_seconds": "1710197134",
        "python_version": "3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]",
        "command_line_arguments": "C:\\Users\\orhun\\anaconda3\\envs\\mlagents\\Scripts\\mlagents-learn results\\0.3.1-01\\configuration.yaml",
        "mlagents_version": "0.30.0",
        "mlagents_envs_version": "0.30.0",
        "communication_protocol_version": "1.5.0",
        "pytorch_version": "2.2.1",
        "numpy_version": "1.21.2",
        "end_time_seconds": "1710198949"
    },
    "total": 1815.0129994,
    "count": 1,
    "self": 4.3324576999996225,
    "children": {
        "run_training.setup": {
            "total": 0.32565069999999974,
            "count": 1,
            "self": 0.32565069999999974
        },
        "TrainerController.start_learning": {
            "total": 1810.3548910000002,
            "count": 1,
            "self": 5.18466249994276,
            "children": {
                "TrainerController._reset_env": {
                    "total": 7.3846858,
                    "count": 1,
                    "self": 7.3846858
                },
                "TrainerController.advance": {
                    "total": 1797.6319754000576,
                    "count": 169184,
                    "self": 5.099149600051078,
                    "children": {
                        "env_step": {
                            "total": 1543.1204925999882,
                            "count": 169184,
                            "self": 641.0611450999899,
                            "children": {
                                "SubprocessEnvManager._take_step": {
                                    "total": 898.2206095999779,
                                    "count": 175137,
                                    "self": 22.4015124999612,
                                    "children": {
                                        "TorchPolicy.evaluate": {
                                            "total": 875.8190971000167,
                                            "count": 172980,
                                            "self": 875.8190971000167
                                        }
                                    }
                                },
                                "workers": {
                                    "total": 3.8387379000204476,
                                    "count": 169183,
                                    "self": 0.0,
                                    "children": {
                                        "worker_root": {
                                            "total": 5404.957726199925,
                                            "count": 175134,
                                            "is_parallel": true,
                                            "self": 1969.5615050998754,
                                            "children": {
                                                "steps_from_proto": {
                                                    "total": 0.0018439000000007866,
                                                    "count": 3,
                                                    "is_parallel": true,
                                                    "self": 0.0005288000000023274,
                                                    "children": {
                                                        "_process_rank_one_or_two_observation": {
                                                            "total": 0.0013150999999984592,
                                                            "count": 6,
                                                            "is_parallel": true,
                                                            "self": 0.0013150999999984592
                                                        }
                                                    }
                                                },
                                                "UnityEnvironment.step": {
                                                    "total": 3435.3943772000493,
                                                    "count": 175134,
                                                    "is_parallel": true,
                                                    "self": 38.24755869992714,
                                                    "children": {
                                                        "UnityEnvironment._generate_step_input": {
                                                            "total": 128.04440000004462,
                                                            "count": 175134,
                                                            "is_parallel": true,
                                                            "self": 128.04440000004462
                                                        },
                                                        "communicator.exchange": {
                                                            "total": 3172.621487200071,
                                                            "count": 175134,
                                                            "is_parallel": true,
                                                            "self": 3172.621487200071
                                                        },
                                                        "steps_from_proto": {
                                                            "total": 96.48093130000655,
                                                            "count": 175134,
                                                            "is_parallel": true,
                                                            "self": 29.072249399956107,
                                                            "children": {
                                                                "_process_rank_one_or_two_observation": {
                                                                    "total": 67.40868190005044,
                                                                    "count": 350268,
                                                                    "is_parallel": true,
                                                                    "self": 67.40868190005044
                                                                }
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        },
                        "trainer_advance": {
                            "total": 249.41233320001825,
                            "count": 169183,
                            "self": 6.242045500056037,
                            "children": {
                                "process_trajectory": {
                                    "total": 110.43381259996228,
                                    "count": 169183,
                                    "self": 110.25548249996224,
                                    "children": {
                                        "RLTrainer._checkpoint": {
                                            "total": 0.17833010000003924,
                                            "count": 1,
                                            "self": 0.17833010000003924
                                        }
                                    }
                                },
                                "_update_policy": {
                                    "total": 132.73647509999995,
                                    "count": 50,
                                    "self": 87.27265400000326,
                                    "children": {
                                        "TorchPPOOptimizer.update": {
                                            "total": 45.463821099996686,
                                            "count": 1500,
                                            "self": 45.463821099996686
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "trainer_threads": {
                    "total": 1.799999836293864e-06,
                    "count": 1,
                    "self": 1.799999836293864e-06
                },
                "TrainerController._save_models": {
                    "total": 0.15356550000001334,
                    "count": 1,
                    "self": 0.004258400000026086,
                    "children": {
                        "RLTrainer._checkpoint": {
                            "total": 0.14930709999998726,
                            "count": 1,
                            "self": 0.14930709999998726
                        }
                    }
                }
            }
        }
    }
}