{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jydQTw71rlIl",
        "-Tssi5gl7fIh",
        "U0CAJK_t7pbw",
        "rGoYOliVTfxh",
        "Yw6O7m-UTnJp"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPDNjpkag+/OU7pvvEztF4X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elymsyr/MLProject/blob/main/Jupyter/RunOnColab_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Environment"
      ],
      "metadata": {
        "id": "rhZLlCW3iZBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/sample_data"
      ],
      "metadata": {
        "id": "JqHMB_sc6aT-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cuda Downgrade"
      ],
      "metadata": {
        "id": "ylB_bZSvbOdz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxHtkoGOHUV5"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get purge \"cuda*\"\n",
        "!rm -rf /usr/local/cuda*\n",
        "!sudo apt-get autoremove\n",
        "!sudo apt-get autoclean"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "s9UMlY9ULU65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install other import packages\n",
        "!sudo apt-get install g++ freeglut3-dev build-essential libx11-dev \\\n",
        "    libxmu-dev libxi-dev libglu1-mesa libglu1-mesa-dev"
      ],
      "metadata": {
        "id": "HDMy3Zzmucjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin\n",
        "!sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda-repo-ubuntu2204-11-8-local_11.8.0-520.61.05-1_amd64.deb\n",
        "!sudo dpkg -i cuda-repo-ubuntu2204-11-8-local_11.8.0-520.61.05-1_amd64.deb\n",
        "!sudo cp /var/cuda-repo-ubuntu2204-11-8-local/cuda-*-keyring.gpg /usr/share/keyrings/\n",
        "!sudo apt-get update\n",
        "!sudo apt-get -y install cuda-11.8"
      ],
      "metadata": {
        "id": "n3R-hJUAMzp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(1, \"/usr/local/cuda-11.8/lib64\")\n",
        "sys.path.insert(1, \"/usr/local/cuda-11.8/bin\")"
      ],
      "metadata": {
        "id": "k3kD6rVx0GJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo 'export PATH=/usr/local/cuda-11.8/bin:$PATH' >> ~/.bashrc\n",
        "!echo 'export LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc\n",
        "!source ~/.bashrc"
      ],
      "metadata": {
        "id": "JAHb0AkvM3Ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -r /usr/local/cuda-xxx/lib64/libcudnn*"
      ],
      "metadata": {
        "id": "tlbpHORdwAU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo wget https://developer.download.nvidia.com/compute/redist/cudnn/v8.6.0/local_installers/11.8/cudnn-linux-x86_64-8.6.0.163_cuda11-archive.tar.xz\n",
        "!sudo tar -xvf /content/cudnn-linux-x86_64-8.6.0.163_cuda11-archive.tar.xz\n",
        "!sudo mv /content/cudnn-linux-x86_64-8.6.0.163_cuda11-archive cuda\n",
        "\n",
        "# copy the following files into the cuda toolkit directory.\n",
        "!sudo cp -P cuda/include/cudnn.h /usr/local/cuda-11.8/include\n",
        "!sudo cp -P cuda/lib/libcudnn* /usr/local/cuda-11.8/lib64/\n",
        "!sudo chmod a+r /usr/local/cuda-11.8/lib64/libcudnn*\n",
        "\n",
        "# Finally, to verify the installation, check\n",
        "!nvidia-smi\n",
        "!nvcc -V"
      ],
      "metadata": {
        "id": "SatYiSBJppf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "id": "v73JeIt80uUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Check if GPU is available\n",
        "!nvcc --version\n",
        "!nvidia-smi\n",
        "if torch.cuda.is_available():\n",
        "    torch.set_default_device('cuda')\n",
        "    # Get the number of available GPUs\n",
        "    num_gpus = torch.cuda.device_count()\n",
        "    if num_gpus > 0:\n",
        "        print(f\"Number of available GPU devices: {num_gpus}\")\n",
        "        # Iterate over available GPUs and print their index and name\n",
        "        for gpu_index in range(num_gpus):\n",
        "            gpu_name = torch.cuda.get_device_name(gpu_index)\n",
        "            print(f\"GPU {gpu_index}: {gpu_name}\")\n",
        "    else:\n",
        "        print(\"No GPU devices available.\")\n",
        "else:\n",
        "    print(\"GPU is not available.\")"
      ],
      "metadata": {
        "id": "HAPC43s3wMHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /usr/src/cudnn_samples_v8/ $HOME\n",
        "! cd  $HOME/cudnn_samples_v8/mnistCUDNN\n",
        "!make clean && make\n",
        "! ./mnistCUDNN"
      ],
      "metadata": {
        "id": "YqvkFWehZWs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "id": "XUHpwc72aI51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Python Version and Packages"
      ],
      "metadata": {
        "id": "5iOfth00ibKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-KnehGu6Czy",
        "outputId": "ed2d03e2-896b-4736-a224-5ad03bdbfc10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downgrade the python version to v3.9. Install necessary packages. You can find it on [github.com/elymsyr/MLProject](https://github.com/elymsyr/MLProject) (/Docs/requirements.txt)"
      ],
      "metadata": {
        "id": "Qf9eFFFWcwof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update -y\n",
        "!sudo apt-get install python3.9\n",
        "\n",
        "#change alternatives\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 1\n",
        "\n",
        "#check python version\n",
        "!python --version\n",
        "\n",
        "# install pip for new python\n",
        "!sudo apt-get install python3.9-distutils\n",
        "!wget https://bootstrap.pypa.io/get-pip.py\n",
        "!python get-pip.py\n",
        "\n",
        "# install colab's dependencies\n",
        "!python -m pip install ipython tensorflow==2.13.0 tensorboard ipython_genutils ipykernel jupyter_console prompt_toolkit httplib2 astor\n",
        "\n",
        "# link to the old google package\n",
        "!ln -s /usr/local/lib/python3.9/dist-packages/google\n",
        "\n",
        "# Change between versions\n",
        "# !sudo update-alternatives --config python3 <RowNumber>\n",
        "\n",
        "!python --version"
      ],
      "metadata": {
        "id": "wzPbOvSDmOlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install --upgrade pip\n",
        "!pip install --upgrade setuptools pip wheel\n",
        "!pip install mlagents==0.30.0\n",
        "!pip install mlagents-envs==0.30.0"
      ],
      "metadata": {
        "id": "Ke_8hbCjhC8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "id": "38Y3m_gipliD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf9ccc45-28f0-4396-ea95-b00e562c8f85"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0\n",
            "    Uninstalling torch-1.11.0:\n",
            "      Successfully uninstalled torch-1.11.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "mlagents 0.30.0 requires torch<=1.11.0,>=1.8.0; platform_system != \"Windows\" and python_version >= \"3.9\", but you have torch 2.2.1+cu118 which is incompatible.\n",
            "tensorflow 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.21.2 which is incompatible.\n",
            "tensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fsspec-2023.4.0 jinja2-3.1.2 mpmath-1.3.0 networkx-3.2.1 nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-8.7.0.84 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.19.3 nvidia-nvtx-cu11-11.8.86 sympy-1.12 torch-2.2.1+cu118 torchaudio-2.2.1+cu118 torchvision-0.17.1+cu118 triton-2.2.0 typing-extensions-4.8.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip3 install torch torchvision torchaudio"
      ],
      "metadata": {
        "id": "1x1v4fG77535"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nvidia-pyindex\n",
        "# !pip install --upgrade nvidia-tensorrt"
      ],
      "metadata": {
        "id": "aLrC1yVnG3Hj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4506882-6ab0-4703-97c0-59167af070a5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-pyindex\n",
            "  Downloading nvidia-pyindex-1.0.9.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: nvidia-pyindex\n",
            "  Building wheel for nvidia-pyindex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-pyindex: filename=nvidia_pyindex-1.0.9-py3-none-any.whl size=8419 sha256=8d5448cd7ee45ef4eb3e686924ad9e4f037fe7a1640373bb7e1fd903cda649d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/63/71/c50214b560fa8c319598c2de3c1616f6d68e1d2c7f17a5e82d\n",
            "Successfully built nvidia-pyindex\n",
            "Installing collected packages: nvidia-pyindex\n",
            "Successfully installed nvidia-pyindex-1.0.9\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install protobuf==3.20.3 install numpy==1.23.5 onnx==1.15.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "id": "bYDF5ZkU8I9g",
        "outputId": "bee32c91-b720-47c1-932d-8f9e4ff59023"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting protobuf==3.20.3\n",
            "  Downloading protobuf-3.20.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (679 bytes)\n",
            "Collecting install\n",
            "  Downloading install-1.3.5-py3-none-any.whl.metadata (925 bytes)\n",
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting onnx==1.15.0\n",
            "  Downloading onnx-1.15.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Downloading protobuf-3.20.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.15.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading install-1.3.5-py3-none-any.whl (3.2 kB)\n",
            "Installing collected packages: protobuf, numpy, install, onnx\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.3\n",
            "    Uninstalling protobuf-4.25.3:\n",
            "      Successfully uninstalled protobuf-4.25.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.2\n",
            "    Uninstalling numpy-1.21.2:\n",
            "      Successfully uninstalled numpy-1.21.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "mlagents 0.30.0 requires torch<=1.11.0,>=1.8.0; platform_system != \"Windows\" and python_version >= \"3.9\", but you have torch 2.2.1+cu118 which is incompatible.\n",
            "mlagents-envs 0.30.0 requires numpy==1.21.2, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed install-1.3.5 numpy-1.23.5 onnx-1.15.0 protobuf-3.20.3\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "89e1b675905c45d09d643ffdd3a393b8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check Environment"
      ],
      "metadata": {
        "id": "xSFmvS2AWmTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Tensorflow-Pytorch GPU"
      ],
      "metadata": {
        "id": "IzJBP6GxdXJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !ln -s /usr/local/lib/python3.9/dist-packages/tensorrt_libs/libnvinfer.so.8 /usr/lib64-nvidia/libnvinfer.so.7\n",
        "# !ln -s /usr/local/lib/python3.9/dist-packages/tensorrt_libs/libnvinfer_plugin.so.8 /usr/lib64-nvidia/libnvinfer_plugin.so.7\n",
        "# !ln -s /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so.12 /usr/lib64-nvidia/libcudart.so.11.0"
      ],
      "metadata": {
        "id": "1vEdkOT0cqoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "# for device in gpu_devices:\n",
        "#     tf.config.experimental.set_memory_growth(device, True)"
      ],
      "metadata": {
        "id": "lZ75KHZH6Mfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.config.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "id": "iuAc13Vc5rYs",
        "outputId": "4ac8e76e-5c67-4a48-eb38-5be155527a84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Old way (deprecated)\n",
        "# torch.set_default_tensor_type(torch.FloatTensor)\n",
        "\n",
        "# New way\n",
        "torch.set_default_dtype(torch.float32)\n",
        "torch.set_default_device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "sH8bNeh_Agj_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Check if GPU is available\n",
        "!nvcc --version\n",
        "!nvidia-smi\n",
        "if torch.cuda.is_available():\n",
        "    torch.set_default_device('cuda')\n",
        "    # Get the number of available GPUs\n",
        "    num_gpus = torch.cuda.device_count()\n",
        "    if num_gpus > 0:\n",
        "        print(f\"Number of available GPU devices: {num_gpus}\")\n",
        "        # Iterate over available GPUs and print their index and name\n",
        "        for gpu_index in range(num_gpus):\n",
        "            gpu_name = torch.cuda.get_device_name(gpu_index)\n",
        "            print(f\"GPU {gpu_index}: {gpu_name}\")\n",
        "    else:\n",
        "        print(\"No GPU devices available.\")\n",
        "else:\n",
        "    print(\"GPU is not available.\")"
      ],
      "metadata": {
        "id": "GnZuyr3ClOQL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8f1583e-2f54-44ff-8914-9b2183b6dc76"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "Fri Mar 15 22:24:00 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8               9W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "Number of available GPU devices: 1\n",
            "GPU 0: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test MLAgents"
      ],
      "metadata": {
        "id": "jydQTw71rlIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mlagents-learn -h"
      ],
      "metadata": {
        "id": "GW5cCfHAiRYW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25ac92bd-4809-425c-800a-f760b3ba4549"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
            "  _C._set_default_tensor_type(t)\n",
            "2024-03-15 22:29:12.497810: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "usage: mlagents-learn [-h] [--env ENV_PATH] [--resume] [--deterministic] [--force]\n",
            "                      [--run-id RUN_ID] [--initialize-from RUN_ID] [--seed SEED] [--inference]\n",
            "                      [--base-port BASE_PORT] [--num-envs NUM_ENVS] [--num-areas NUM_AREAS]\n",
            "                      [--debug] [--env-args ...] [--max-lifetime-restarts MAX_LIFETIME_RESTARTS]\n",
            "                      [--restarts-rate-limit-n RESTARTS_RATE_LIMIT_N]\n",
            "                      [--restarts-rate-limit-period-s RESTARTS_RATE_LIMIT_PERIOD_S] [--torch]\n",
            "                      [--tensorflow] [--results-dir RESULTS_DIR] [--width WIDTH] [--height HEIGHT]\n",
            "                      [--quality-level QUALITY_LEVEL] [--time-scale TIME_SCALE]\n",
            "                      [--target-frame-rate TARGET_FRAME_RATE]\n",
            "                      [--capture-frame-rate CAPTURE_FRAME_RATE] [--no-graphics]\n",
            "                      [--torch-device DEVICE]\n",
            "                      [trainer_config_path]\n",
            "\n",
            "positional arguments:\n",
            "  trainer_config_path\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --env ENV_PATH        Path to the Unity executable to train (default: None)\n",
            "  --resume              Whether to resume training from a checkpoint. Specify a --run-id to use\n",
            "                        this option. If set, the training code loads an already trained model to\n",
            "                        initialize the neural network before resuming training. This option is\n",
            "                        only valid when the models exist, and have the same behavior names as the\n",
            "                        current agents in your scene. (default: False)\n",
            "  --deterministic       Whether to select actions deterministically in policy. `dist.mean` for\n",
            "                        continuous action space, and `dist.argmax` for deterministic action space\n",
            "                        (default: False)\n",
            "  --force               Whether to force-overwrite this run-id's existing summary and model data.\n",
            "                        (Without this flag, attempting to train a model with a run-id that has\n",
            "                        been used before will throw an error. (default: False)\n",
            "  --run-id RUN_ID       The identifier for the training run. This identifier is used to name the\n",
            "                        subdirectories in which the trained model and summary statistics are saved\n",
            "                        as well as the saved model itself. If you use TensorBoard to view the\n",
            "                        training statistics, always set a unique run-id for each training run.\n",
            "                        (The statistics for all runs with the same id are combined as if they were\n",
            "                        produced by a the same session.) (default: ppo)\n",
            "  --initialize-from RUN_ID\n",
            "                        Specify a previously saved run ID from which to initialize the model from.\n",
            "                        This can be used, for instance, to fine-tune an existing model on a new\n",
            "                        environment. Note that the previously saved models must have the same\n",
            "                        behavior parameters as your current environment. (default: None)\n",
            "  --seed SEED           A number to use as a seed for the random number generator used by the\n",
            "                        training code (default: -1)\n",
            "  --inference           Whether to run in Python inference mode (i.e. no training). Use with\n",
            "                        --resume to load a model trained with an existing run ID. (default: False)\n",
            "  --base-port BASE_PORT\n",
            "                        The starting port for environment communication. Each concurrent Unity\n",
            "                        environment instance will get assigned a port sequentially, starting from\n",
            "                        the base-port. Each instance will use the port (base_port + worker_id),\n",
            "                        where the worker_id is sequential IDs given to each instance from 0 to\n",
            "                        (num_envs - 1). Note that when training using the Editor rather than an\n",
            "                        executable, the base port will be ignored. (default: 5005)\n",
            "  --num-envs NUM_ENVS   The number of concurrent Unity environment instances to collect\n",
            "                        experiences from when training (default: 1)\n",
            "  --num-areas NUM_AREAS\n",
            "                        The number of parallel training areas in each Unity environment instance.\n",
            "                        (default: 1)\n",
            "  --debug               Whether to enable debug-level logging for some parts of the code (default:\n",
            "                        False)\n",
            "  --env-args ...        Arguments passed to the Unity executable. Be aware that the standalone\n",
            "                        build will also process these as Unity Command Line Arguments. You should\n",
            "                        choose different argument names if you want to create environment-specific\n",
            "                        arguments. All arguments after this flag will be passed to the executable.\n",
            "                        (default: None)\n",
            "  --max-lifetime-restarts MAX_LIFETIME_RESTARTS\n",
            "                        The max number of times a single Unity executable can crash over its\n",
            "                        lifetime before ml-agents exits. Can be set to -1 if no limit is desired.\n",
            "                        (default: 10)\n",
            "  --restarts-rate-limit-n RESTARTS_RATE_LIMIT_N\n",
            "                        The maximum number of times a single Unity executable can crash over a\n",
            "                        period of time (period set in restarts-rate-limit-period-s). Can be set to\n",
            "                        -1 to not use rate limiting with restarts. (default: 1)\n",
            "  --restarts-rate-limit-period-s RESTARTS_RATE_LIMIT_PERIOD_S\n",
            "                        The period of time --restarts-rate-limit-n applies to. (default: 60)\n",
            "  --torch               (Removed) Use the PyTorch framework. (default: False)\n",
            "  --tensorflow          (Removed) Use the TensorFlow framework. (default: False)\n",
            "  --results-dir RESULTS_DIR\n",
            "                        Results base directory (default: results)\n",
            "\n",
            "Engine Configuration:\n",
            "  --width WIDTH         The width of the executable window of the environment(s) in pixels\n",
            "                        (ignored for editor training). (default: 84)\n",
            "  --height HEIGHT       The height of the executable window of the environment(s) in pixels\n",
            "                        (ignored for editor training) (default: 84)\n",
            "  --quality-level QUALITY_LEVEL\n",
            "                        The quality level of the environment(s). Equivalent to calling\n",
            "                        QualitySettings.SetQualityLevel in Unity. (default: 5)\n",
            "  --time-scale TIME_SCALE\n",
            "                        The time scale of the Unity environment(s). Equivalent to setting\n",
            "                        Time.timeScale in Unity. (default: 20)\n",
            "  --target-frame-rate TARGET_FRAME_RATE\n",
            "                        The target frame rate of the Unity environment(s). Equivalent to setting\n",
            "                        Application.targetFrameRate in Unity. (default: -1)\n",
            "  --capture-frame-rate CAPTURE_FRAME_RATE\n",
            "                        The capture frame rate of the Unity environment(s). Equivalent to setting\n",
            "                        Time.captureFramerate in Unity. (default: 60)\n",
            "  --no-graphics         Whether to run the Unity executable in no-graphics mode (i.e. without\n",
            "                        initializing the graphics driver. Use this only if your agents don't use\n",
            "                        visual observations. (default: False)\n",
            "\n",
            "Torch Configuration:\n",
            "  --torch-device DEVICE\n",
            "                        Settings for the default torch.device used in training, for example,\n",
            "                        \"cpu\", \"cuda\", or \"cuda:0\" (default: None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Build"
      ],
      "metadata": {
        "id": "-Tssi5gl7fIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --branch main https://github.com/elymsyr/MLProject.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEa2Kj4G6WZ-",
        "outputId": "f8884e0c-4bd6-4e19-f934-0141c5059558"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MLProject'...\n",
            "remote: Enumerating objects: 1187, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 1187 (delta 14), reused 12 (delta 12), pack-reused 1170\u001b[K\n",
            "Receiving objects: 100% (1187/1187), 435.89 MiB | 16.16 MiB/s, done.\n",
            "Resolving deltas: 100% (631/631), done.\n",
            "Updating files: 100% (527/527), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the permissions\n",
        "!chmod -R 755 /content/MLProject/Jupyter/b061/b061.x86_64\n",
        "!chmod -R 755 /content/MLProject/Jupyter/b061/UnityPlayer.so\n",
        "!ls -l /content/MLProject/Jupyter/b061"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53mAtnX76kw3",
        "outputId": "9e810923-e3d5-4608-843c-cf6ad0aa4b0b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 112816\n",
            "drwxr-xr-x 3 root root     4096 Mar 15 22:29 b061_BurstDebugInformation_DoNotShip\n",
            "drwxr-xr-x 6 root root     4096 Mar 15 22:29 b061_Data\n",
            "-rw-r--r-- 1 root root    16208 Mar 15 22:29 b061_s.debug\n",
            "-rwxr-xr-x 1 root root    15096 Mar 15 22:29 b061.x86_64\n",
            "-rw-r--r-- 1 root root 67164248 Mar 15 22:29 UnityPlayer_s.debug\n",
            "-rwxr-xr-x 1 root root 48314296 Mar 15 22:29 UnityPlayer.so\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create config.yaml"
      ],
      "metadata": {
        "id": "U0CAJK_t7pbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config.yaml\n",
        "\n",
        "default_settings: null\n",
        "behaviors:\n",
        "  AgentBehavior:\n",
        "    trainer_type: ppo\n",
        "    hyperparameters:\n",
        "      batch_size: 1024\n",
        "      buffer_size: 10240\n",
        "      learning_rate: 0.0003\n",
        "      beta: 0.005\n",
        "      epsilon: 0.2\n",
        "      lambd: 0.95\n",
        "      num_epoch: 3\n",
        "      shared_critic: false\n",
        "      learning_rate_schedule: linear\n",
        "      beta_schedule: linear\n",
        "      epsilon_schedule: linear\n",
        "    network_settings:\n",
        "      normalize: false\n",
        "      hidden_units: 256\n",
        "      num_layers: 3\n",
        "      vis_encode_type: simple\n",
        "      memory: null\n",
        "      goal_conditioning_type: hyper\n",
        "      deterministic: false\n",
        "    reward_signals:\n",
        "      extrinsic:\n",
        "        gamma: 0.99\n",
        "        strength: 1.0\n",
        "        network_settings:\n",
        "          normalize: false\n",
        "          hidden_units: 256\n",
        "          num_layers: 3\n",
        "          vis_encode_type: simple\n",
        "          memory: null\n",
        "          goal_conditioning_type: hyper\n",
        "          deterministic: false\n",
        "    init_path: null\n",
        "    keep_checkpoints: 5\n",
        "    checkpoint_interval: 500000\n",
        "    max_steps: 10000000\n",
        "    time_horizon: 64\n",
        "    summary_freq: 10000\n",
        "    threaded: false\n",
        "    self_play: null\n",
        "    behavioral_cloning: null\n",
        "env_settings:\n",
        "  env_path: /content/MLProject/Jupyter/b061/b061.x86_64\n",
        "  env_args: null\n",
        "  base_port: 5004\n",
        "  num_envs: 4\n",
        "  num_areas: 1\n",
        "  seed: -1\n",
        "  max_lifetime_restarts: 10\n",
        "  restarts_rate_limit_n: 1\n",
        "  restarts_rate_limit_period_s: 60\n",
        "engine_settings:\n",
        "  width: 84\n",
        "  height: 84\n",
        "  quality_level: 5\n",
        "  time_scale: 20.0\n",
        "  target_frame_rate: -1\n",
        "  capture_frame_rate: 60\n",
        "  no_graphics: true\n",
        "environment_parameters: null\n",
        "checkpoint_settings:\n",
        "  run_id: ColabTest\n",
        "  initialize_from:\n",
        "  load_model: false\n",
        "  resume: false\n",
        "  force: false\n",
        "  train_model: false\n",
        "  inference: false\n",
        "  results_dir: results\n",
        "torch_settings:\n",
        "  device: cuda\n",
        "debug: false"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyVmeCgc6sUG",
        "outputId": "67fbee5c-c196-4b37-befa-72e9811c1cc7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing config.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tenserboard and Train Process"
      ],
      "metadata": {
        "id": "9IL5Pm_s-O2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorboard"
      ],
      "metadata": {
        "id": "rGoYOliVTfxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir results\n",
        "%reload_ext tensorboard"
      ],
      "metadata": {
        "id": "WwITCL-w-RNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Delete previous trains"
      ],
      "metadata": {
        "id": "Yw6O7m-UTnJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/results # if necessary"
      ],
      "metadata": {
        "id": "u6VC2WGfRyR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "Lj7Q4BsrTkkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mlagents-learn /content/config.yaml --env /content/MLProject/Jupyter/b061/b061.x86_64 --resume"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dZ7p5Pw7Yey",
        "outputId": "2955268c-a314-4644-8ce5-bc8783396294"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
            "  _C._set_default_tensor_type(t)\n",
            "2024-03-16 12:42:11.330095: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "            ┐  ╖\n",
            "        ╓╖╬│╡  ││╬╖╖\n",
            "    ╓╖╬│││││┘  ╬│││││╬╖\n",
            " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
            " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
            " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
            " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
            " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
            " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
            " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
            "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
            "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
            "          ╙╬╬╬╣╣╣╜\n",
            "             ╙\n",
            "        \n",
            " Version information:\n",
            "  ml-agents: 0.30.0,\n",
            "  ml-agents-envs: 0.30.0,\n",
            "  Communicator API: 1.5.0,\n",
            "  PyTorch: 2.2.1+cu118\n",
            "[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0\n",
            "[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0\n",
            "[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0\n",
            "[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0\n",
            "[INFO] Connected new brain: AgentBehavior?team=0\n",
            "[INFO] Connected new brain: AgentBehavior?team=0\n",
            "[INFO] Connected new brain: AgentBehavior?team=0\n",
            "[INFO] Connected new brain: AgentBehavior?team=0\n",
            "[INFO] Hyperparameters for behavior name AgentBehavior: \n",
            "\ttrainer_type:\tppo\n",
            "\thyperparameters:\t\n",
            "\t  batch_size:\t1024\n",
            "\t  buffer_size:\t10240\n",
            "\t  learning_rate:\t0.0003\n",
            "\t  beta:\t0.005\n",
            "\t  epsilon:\t0.2\n",
            "\t  lambd:\t0.95\n",
            "\t  num_epoch:\t3\n",
            "\t  shared_critic:\tFalse\n",
            "\t  learning_rate_schedule:\tlinear\n",
            "\t  beta_schedule:\tlinear\n",
            "\t  epsilon_schedule:\tlinear\n",
            "\tnetwork_settings:\t\n",
            "\t  normalize:\tFalse\n",
            "\t  hidden_units:\t256\n",
            "\t  num_layers:\t3\n",
            "\t  vis_encode_type:\tsimple\n",
            "\t  memory:\tNone\n",
            "\t  goal_conditioning_type:\thyper\n",
            "\t  deterministic:\tFalse\n",
            "\treward_signals:\t\n",
            "\t  extrinsic:\t\n",
            "\t    gamma:\t0.99\n",
            "\t    strength:\t1.0\n",
            "\t    network_settings:\t\n",
            "\t      normalize:\tFalse\n",
            "\t      hidden_units:\t256\n",
            "\t      num_layers:\t3\n",
            "\t      vis_encode_type:\tsimple\n",
            "\t      memory:\tNone\n",
            "\t      goal_conditioning_type:\thyper\n",
            "\t      deterministic:\tFalse\n",
            "\tinit_path:\tNone\n",
            "\tkeep_checkpoints:\t5\n",
            "\tcheckpoint_interval:\t500000\n",
            "\tmax_steps:\t10000000\n",
            "\ttime_horizon:\t64\n",
            "\tsummary_freq:\t10000\n",
            "\tthreaded:\tFalse\n",
            "\tself_play:\tNone\n",
            "\tbehavioral_cloning:\tNone\n",
            "[INFO] Resuming from results/ColabTest/AgentBehavior.\n",
            "[INFO] Resuming training from step 7499942.\n",
            "[INFO] AgentBehavior. Step: 7500000. Time Elapsed: 4.173 s. No episode was completed since last summary. Training.\n",
            "[INFO] Exported results/ColabTest/AgentBehavior/AgentBehavior-7499942.onnx\n",
            "[INFO] AgentBehavior. Step: 7510000. Time Elapsed: 68.233 s. Mean Reward: 1.969. Std of Reward: 2.363. Training.\n",
            "[INFO] AgentBehavior. Step: 7520000. Time Elapsed: 131.670 s. Mean Reward: 1.255. Std of Reward: 2.627. Training.\n",
            "[INFO] AgentBehavior. Step: 7530000. Time Elapsed: 195.129 s. Mean Reward: 1.124. Std of Reward: 2.295. Training.\n",
            "[INFO] AgentBehavior. Step: 7540000. Time Elapsed: 258.574 s. Mean Reward: 1.445. Std of Reward: 2.247. Training.\n",
            "[INFO] AgentBehavior. Step: 7550000. Time Elapsed: 323.324 s. Mean Reward: 1.354. Std of Reward: 2.637. Training.\n",
            "[INFO] AgentBehavior. Step: 7560000. Time Elapsed: 385.514 s. Mean Reward: 1.576. Std of Reward: 2.370. Training.\n",
            "[INFO] AgentBehavior. Step: 7570000. Time Elapsed: 448.461 s. Mean Reward: 1.493. Std of Reward: 2.465. Training.\n",
            "[INFO] AgentBehavior. Step: 7580000. Time Elapsed: 512.599 s. Mean Reward: 1.535. Std of Reward: 2.350. Training.\n",
            "[INFO] AgentBehavior. Step: 7590000. Time Elapsed: 577.411 s. Mean Reward: 1.363. Std of Reward: 2.354. Training.\n",
            "[INFO] AgentBehavior. Step: 7600000. Time Elapsed: 641.315 s. Mean Reward: 1.629. Std of Reward: 2.392. Training.\n",
            "[INFO] AgentBehavior. Step: 7610000. Time Elapsed: 704.127 s. Mean Reward: 0.950. Std of Reward: 2.289. Training.\n",
            "[INFO] AgentBehavior. Step: 7620000. Time Elapsed: 768.720 s. Mean Reward: 1.793. Std of Reward: 2.407. Training.\n",
            "[INFO] AgentBehavior. Step: 7630000. Time Elapsed: 831.827 s. Mean Reward: 1.468. Std of Reward: 2.288. Training.\n",
            "[INFO] AgentBehavior. Step: 7640000. Time Elapsed: 896.057 s. Mean Reward: 1.180. Std of Reward: 2.257. Training.\n",
            "[INFO] AgentBehavior. Step: 7650000. Time Elapsed: 959.837 s. Mean Reward: 1.140. Std of Reward: 2.405. Training.\n",
            "[INFO] AgentBehavior. Step: 7660000. Time Elapsed: 1024.240 s. Mean Reward: 1.326. Std of Reward: 2.457. Training.\n",
            "[INFO] AgentBehavior. Step: 7670000. Time Elapsed: 1088.894 s. Mean Reward: 1.543. Std of Reward: 2.387. Training.\n",
            "[INFO] AgentBehavior. Step: 7680000. Time Elapsed: 1152.801 s. Mean Reward: 1.275. Std of Reward: 2.565. Training.\n",
            "[INFO] AgentBehavior. Step: 7690000. Time Elapsed: 1218.426 s. Mean Reward: 1.553. Std of Reward: 2.423. Training.\n",
            "[INFO] AgentBehavior. Step: 7700000. Time Elapsed: 1284.285 s. Mean Reward: 1.275. Std of Reward: 2.390. Training.\n",
            "[INFO] AgentBehavior. Step: 7710000. Time Elapsed: 1353.452 s. Mean Reward: 0.852. Std of Reward: 2.340. Training.\n",
            "[INFO] AgentBehavior. Step: 7720000. Time Elapsed: 1420.533 s. Mean Reward: 1.634. Std of Reward: 2.213. Training.\n",
            "[INFO] AgentBehavior. Step: 7730000. Time Elapsed: 1483.632 s. Mean Reward: 1.058. Std of Reward: 2.380. Training.\n",
            "[INFO] AgentBehavior. Step: 7740000. Time Elapsed: 1549.830 s. Mean Reward: 1.351. Std of Reward: 2.590. Training.\n",
            "[INFO] AgentBehavior. Step: 7750000. Time Elapsed: 1615.697 s. Mean Reward: 1.358. Std of Reward: 2.344. Training.\n",
            "[INFO] AgentBehavior. Step: 7760000. Time Elapsed: 1680.905 s. Mean Reward: 1.287. Std of Reward: 2.372. Training.\n",
            "[INFO] AgentBehavior. Step: 7770000. Time Elapsed: 1745.433 s. Mean Reward: 1.604. Std of Reward: 2.287. Training.\n",
            "[INFO] AgentBehavior. Step: 7780000. Time Elapsed: 1811.183 s. Mean Reward: 1.454. Std of Reward: 2.367. Training.\n",
            "[INFO] AgentBehavior. Step: 7790000. Time Elapsed: 1878.858 s. Mean Reward: 1.250. Std of Reward: 2.330. Training.\n",
            "[INFO] AgentBehavior. Step: 7800000. Time Elapsed: 1948.191 s. Mean Reward: 0.952. Std of Reward: 2.493. Training.\n",
            "[INFO] AgentBehavior. Step: 7810000. Time Elapsed: 2012.315 s. Mean Reward: 1.479. Std of Reward: 2.511. Training.\n",
            "[INFO] AgentBehavior. Step: 7820000. Time Elapsed: 2075.435 s. Mean Reward: 1.222. Std of Reward: 2.306. Training.\n",
            "[INFO] AgentBehavior. Step: 7830000. Time Elapsed: 2140.778 s. Mean Reward: 1.085. Std of Reward: 2.369. Training.\n",
            "[INFO] AgentBehavior. Step: 7840000. Time Elapsed: 2204.181 s. Mean Reward: 1.077. Std of Reward: 2.541. Training.\n",
            "[INFO] AgentBehavior. Step: 7850000. Time Elapsed: 2268.561 s. Mean Reward: 1.360. Std of Reward: 2.352. Training.\n",
            "[INFO] AgentBehavior. Step: 7860000. Time Elapsed: 2332.279 s. Mean Reward: 1.427. Std of Reward: 2.394. Training.\n",
            "[INFO] AgentBehavior. Step: 7870000. Time Elapsed: 2396.779 s. Mean Reward: 1.094. Std of Reward: 2.141. Training.\n",
            "[INFO] AgentBehavior. Step: 7880000. Time Elapsed: 2459.303 s. Mean Reward: 1.590. Std of Reward: 2.299. Training.\n",
            "[INFO] AgentBehavior. Step: 7890000. Time Elapsed: 2519.869 s. Mean Reward: 1.482. Std of Reward: 2.468. Training.\n",
            "[INFO] AgentBehavior. Step: 7900000. Time Elapsed: 2583.028 s. Mean Reward: 1.616. Std of Reward: 2.563. Training.\n",
            "[INFO] AgentBehavior. Step: 7910000. Time Elapsed: 2646.051 s. Mean Reward: 1.519. Std of Reward: 2.437. Training.\n",
            "[INFO] AgentBehavior. Step: 7920000. Time Elapsed: 2709.347 s. Mean Reward: 1.214. Std of Reward: 2.635. Training.\n",
            "[INFO] AgentBehavior. Step: 7930000. Time Elapsed: 2770.912 s. Mean Reward: 1.240. Std of Reward: 2.482. Training.\n",
            "[INFO] AgentBehavior. Step: 7940000. Time Elapsed: 2834.188 s. Mean Reward: 1.457. Std of Reward: 2.479. Training.\n",
            "[INFO] AgentBehavior. Step: 7950000. Time Elapsed: 2897.369 s. Mean Reward: 1.052. Std of Reward: 2.412. Training.\n",
            "[INFO] AgentBehavior. Step: 7960000. Time Elapsed: 2960.005 s. Mean Reward: 1.433. Std of Reward: 2.244. Training.\n",
            "[INFO] AgentBehavior. Step: 7970000. Time Elapsed: 3022.530 s. Mean Reward: 1.478. Std of Reward: 2.223. Training.\n",
            "[INFO] AgentBehavior. Step: 7980000. Time Elapsed: 3084.662 s. Mean Reward: 1.534. Std of Reward: 2.255. Training.\n",
            "[INFO] AgentBehavior. Step: 7990000. Time Elapsed: 3148.625 s. Mean Reward: 1.406. Std of Reward: 2.482. Training.\n",
            "[INFO] AgentBehavior. Step: 8000000. Time Elapsed: 3211.003 s. Mean Reward: 1.482. Std of Reward: 2.241. Training.\n",
            "[INFO] Exported results/ColabTest/AgentBehavior/AgentBehavior-7999966.onnx\n",
            "[INFO] AgentBehavior. Step: 8010000. Time Elapsed: 3271.895 s. Mean Reward: 1.047. Std of Reward: 2.191. Training.\n",
            "[INFO] AgentBehavior. Step: 8020000. Time Elapsed: 3335.434 s. Mean Reward: 1.222. Std of Reward: 2.522. Training.\n",
            "[INFO] AgentBehavior. Step: 8030000. Time Elapsed: 3397.043 s. Mean Reward: 1.773. Std of Reward: 2.277. Training.\n",
            "[INFO] AgentBehavior. Step: 8040000. Time Elapsed: 3460.421 s. Mean Reward: 1.757. Std of Reward: 2.205. Training.\n",
            "[INFO] AgentBehavior. Step: 8050000. Time Elapsed: 3520.797 s. Mean Reward: 1.456. Std of Reward: 2.304. Training.\n",
            "[INFO] AgentBehavior. Step: 8060000. Time Elapsed: 3582.884 s. Mean Reward: 0.848. Std of Reward: 2.247. Training.\n",
            "[INFO] AgentBehavior. Step: 8070000. Time Elapsed: 3646.312 s. Mean Reward: 1.506. Std of Reward: 2.417. Training.\n",
            "[INFO] AgentBehavior. Step: 8080000. Time Elapsed: 3707.727 s. Mean Reward: 1.412. Std of Reward: 2.320. Training.\n",
            "[INFO] AgentBehavior. Step: 8090000. Time Elapsed: 3770.369 s. Mean Reward: 1.471. Std of Reward: 2.392. Training.\n",
            "[INFO] AgentBehavior. Step: 8100000. Time Elapsed: 3831.397 s. Mean Reward: 1.564. Std of Reward: 2.302. Training.\n",
            "[INFO] AgentBehavior. Step: 8110000. Time Elapsed: 3893.193 s. Mean Reward: 1.486. Std of Reward: 2.341. Training.\n",
            "[INFO] AgentBehavior. Step: 8120000. Time Elapsed: 3956.168 s. Mean Reward: 0.794. Std of Reward: 2.277. Training.\n",
            "[INFO] AgentBehavior. Step: 8130000. Time Elapsed: 4016.557 s. Mean Reward: 1.901. Std of Reward: 2.634. Training.\n",
            "[INFO] AgentBehavior. Step: 8140000. Time Elapsed: 4079.962 s. Mean Reward: 1.558. Std of Reward: 2.393. Training.\n",
            "[INFO] AgentBehavior. Step: 8150000. Time Elapsed: 4142.150 s. Mean Reward: 1.647. Std of Reward: 2.337. Training.\n",
            "[INFO] AgentBehavior. Step: 8160000. Time Elapsed: 4202.725 s. Mean Reward: 1.531. Std of Reward: 2.494. Training.\n",
            "[INFO] AgentBehavior. Step: 8170000. Time Elapsed: 4266.169 s. Mean Reward: 1.367. Std of Reward: 2.468. Training.\n",
            "[INFO] AgentBehavior. Step: 8180000. Time Elapsed: 4328.062 s. Mean Reward: 1.721. Std of Reward: 2.341. Training.\n",
            "[INFO] AgentBehavior. Step: 8190000. Time Elapsed: 4388.451 s. Mean Reward: 1.430. Std of Reward: 2.402. Training.\n",
            "[INFO] AgentBehavior. Step: 8200000. Time Elapsed: 4450.693 s. Mean Reward: 1.740. Std of Reward: 2.340. Training.\n",
            "[INFO] AgentBehavior. Step: 8210000. Time Elapsed: 4512.417 s. Mean Reward: 1.073. Std of Reward: 2.455. Training.\n",
            "[INFO] AgentBehavior. Step: 8220000. Time Elapsed: 4575.298 s. Mean Reward: 1.438. Std of Reward: 2.455. Training.\n",
            "[INFO] AgentBehavior. Step: 8230000. Time Elapsed: 4636.418 s. Mean Reward: 1.470. Std of Reward: 2.428. Training.\n",
            "[INFO] AgentBehavior. Step: 8240000. Time Elapsed: 4697.086 s. Mean Reward: 1.902. Std of Reward: 2.382. Training.\n",
            "[INFO] AgentBehavior. Step: 8250000. Time Elapsed: 4760.456 s. Mean Reward: 1.304. Std of Reward: 2.421. Training.\n",
            "[INFO] AgentBehavior. Step: 8260000. Time Elapsed: 4819.066 s. Mean Reward: 1.235. Std of Reward: 2.446. Training.\n",
            "[INFO] AgentBehavior. Step: 8270000. Time Elapsed: 4880.872 s. Mean Reward: 1.046. Std of Reward: 2.351. Training.\n",
            "[INFO] AgentBehavior. Step: 8280000. Time Elapsed: 4941.908 s. Mean Reward: 1.366. Std of Reward: 2.332. Training.\n",
            "[INFO] AgentBehavior. Step: 8290000. Time Elapsed: 5004.301 s. Mean Reward: 1.373. Std of Reward: 2.395. Training.\n",
            "[INFO] AgentBehavior. Step: 8300000. Time Elapsed: 5066.961 s. Mean Reward: 1.654. Std of Reward: 2.327. Training.\n",
            "[INFO] AgentBehavior. Step: 8310000. Time Elapsed: 5128.730 s. Mean Reward: 1.651. Std of Reward: 2.418. Training.\n",
            "[INFO] AgentBehavior. Step: 8320000. Time Elapsed: 5190.885 s. Mean Reward: 1.449. Std of Reward: 2.291. Training.\n",
            "[INFO] AgentBehavior. Step: 8330000. Time Elapsed: 5253.790 s. Mean Reward: 1.446. Std of Reward: 2.326. Training.\n",
            "[INFO] AgentBehavior. Step: 8340000. Time Elapsed: 5315.138 s. Mean Reward: 1.435. Std of Reward: 2.318. Training.\n",
            "[INFO] AgentBehavior. Step: 8350000. Time Elapsed: 5379.142 s. Mean Reward: 1.642. Std of Reward: 2.265. Training.\n",
            "[INFO] AgentBehavior. Step: 8360000. Time Elapsed: 5440.817 s. Mean Reward: 1.294. Std of Reward: 2.596. Training.\n",
            "[INFO] AgentBehavior. Step: 8370000. Time Elapsed: 5502.213 s. Mean Reward: 1.516. Std of Reward: 2.624. Training.\n",
            "[INFO] AgentBehavior. Step: 8380000. Time Elapsed: 5564.804 s. Mean Reward: 1.536. Std of Reward: 2.326. Training.\n",
            "[INFO] AgentBehavior. Step: 8390000. Time Elapsed: 5626.760 s. Mean Reward: 1.334. Std of Reward: 2.281. Training.\n",
            "[INFO] AgentBehavior. Step: 8400000. Time Elapsed: 5688.633 s. Mean Reward: 1.215. Std of Reward: 2.299. Training.\n",
            "[INFO] AgentBehavior. Step: 8410000. Time Elapsed: 5751.076 s. Mean Reward: 1.279. Std of Reward: 2.300. Training.\n",
            "[INFO] AgentBehavior. Step: 8420000. Time Elapsed: 5812.693 s. Mean Reward: 1.330. Std of Reward: 2.384. Training.\n",
            "[INFO] AgentBehavior. Step: 8430000. Time Elapsed: 5875.694 s. Mean Reward: 1.863. Std of Reward: 2.404. Training.\n",
            "[INFO] AgentBehavior. Step: 8440000. Time Elapsed: 5937.525 s. Mean Reward: 1.310. Std of Reward: 2.526. Training.\n",
            "[INFO] AgentBehavior. Step: 8450000. Time Elapsed: 5997.775 s. Mean Reward: 1.131. Std of Reward: 2.530. Training.\n",
            "[INFO] AgentBehavior. Step: 8460000. Time Elapsed: 6060.864 s. Mean Reward: 1.252. Std of Reward: 2.214. Training.\n",
            "[INFO] AgentBehavior. Step: 8470000. Time Elapsed: 6122.818 s. Mean Reward: 1.742. Std of Reward: 2.397. Training.\n",
            "[INFO] AgentBehavior. Step: 8480000. Time Elapsed: 6184.385 s. Mean Reward: 0.821. Std of Reward: 2.389. Training.\n",
            "[INFO] AgentBehavior. Step: 8490000. Time Elapsed: 6246.927 s. Mean Reward: 1.463. Std of Reward: 2.364. Training.\n",
            "[INFO] AgentBehavior. Step: 8500000. Time Elapsed: 6307.993 s. Mean Reward: 1.134. Std of Reward: 2.354. Training.\n",
            "[INFO] Exported results/ColabTest/AgentBehavior/AgentBehavior-8499962.onnx\n",
            "[INFO] AgentBehavior. Step: 8510000. Time Elapsed: 6371.630 s. Mean Reward: 1.599. Std of Reward: 2.353. Training.\n",
            "[INFO] AgentBehavior. Step: 8520000. Time Elapsed: 6433.741 s. Mean Reward: 1.064. Std of Reward: 2.318. Training.\n",
            "[INFO] AgentBehavior. Step: 8530000. Time Elapsed: 6494.251 s. Mean Reward: 0.905. Std of Reward: 2.351. Training.\n",
            "[INFO] AgentBehavior. Step: 8540000. Time Elapsed: 6557.472 s. Mean Reward: 1.131. Std of Reward: 2.381. Training.\n",
            "[INFO] AgentBehavior. Step: 8550000. Time Elapsed: 6619.035 s. Mean Reward: 1.707. Std of Reward: 2.300. Training.\n",
            "[INFO] AgentBehavior. Step: 8560000. Time Elapsed: 6680.778 s. Mean Reward: 1.337. Std of Reward: 2.272. Training.\n",
            "[INFO] AgentBehavior. Step: 8570000. Time Elapsed: 6743.207 s. Mean Reward: 1.540. Std of Reward: 2.504. Training.\n",
            "[INFO] AgentBehavior. Step: 8580000. Time Elapsed: 6804.802 s. Mean Reward: 1.283. Std of Reward: 2.525. Training.\n",
            "[INFO] AgentBehavior. Step: 8590000. Time Elapsed: 6866.399 s. Mean Reward: 1.138. Std of Reward: 2.357. Training.\n",
            "[INFO] AgentBehavior. Step: 8600000. Time Elapsed: 6929.243 s. Mean Reward: 1.691. Std of Reward: 2.471. Training.\n",
            "[INFO] AgentBehavior. Step: 8610000. Time Elapsed: 6990.140 s. Mean Reward: 1.420. Std of Reward: 2.489. Training.\n",
            "[INFO] AgentBehavior. Step: 8620000. Time Elapsed: 7053.579 s. Mean Reward: 1.607. Std of Reward: 2.422. Training.\n",
            "[INFO] AgentBehavior. Step: 8630000. Time Elapsed: 7114.792 s. Mean Reward: 1.664. Std of Reward: 2.269. Training.\n",
            "[INFO] AgentBehavior. Step: 8640000. Time Elapsed: 7173.612 s. Mean Reward: 1.329. Std of Reward: 2.295. Training.\n",
            "[INFO] AgentBehavior. Step: 8650000. Time Elapsed: 7236.319 s. Mean Reward: 1.506. Std of Reward: 2.308. Training.\n",
            "[INFO] AgentBehavior. Step: 8660000. Time Elapsed: 7297.350 s. Mean Reward: 1.718. Std of Reward: 2.397. Training.\n",
            "[INFO] AgentBehavior. Step: 8670000. Time Elapsed: 7357.496 s. Mean Reward: 2.107. Std of Reward: 2.428. Training.\n",
            "[INFO] AgentBehavior. Step: 8680000. Time Elapsed: 7420.300 s. Mean Reward: 1.342. Std of Reward: 2.420. Training.\n",
            "[INFO] AgentBehavior. Step: 8690000. Time Elapsed: 7481.202 s. Mean Reward: 1.595. Std of Reward: 2.393. Training.\n",
            "[INFO] AgentBehavior. Step: 8700000. Time Elapsed: 7543.247 s. Mean Reward: 1.633. Std of Reward: 2.371. Training.\n",
            "[INFO] AgentBehavior. Step: 8710000. Time Elapsed: 7606.009 s. Mean Reward: 1.443. Std of Reward: 2.322. Training.\n",
            "[INFO] AgentBehavior. Step: 8720000. Time Elapsed: 7668.871 s. Mean Reward: 1.161. Std of Reward: 2.359. Training.\n",
            "[INFO] AgentBehavior. Step: 8730000. Time Elapsed: 7731.287 s. Mean Reward: 1.442. Std of Reward: 2.570. Training.\n",
            "[INFO] AgentBehavior. Step: 8740000. Time Elapsed: 7796.276 s. Mean Reward: 1.776. Std of Reward: 2.432. Training.\n",
            "[INFO] AgentBehavior. Step: 8750000. Time Elapsed: 7859.566 s. Mean Reward: 1.505. Std of Reward: 2.291. Training.\n",
            "[INFO] AgentBehavior. Step: 8760000. Time Elapsed: 7924.721 s. Mean Reward: 1.295. Std of Reward: 2.259. Training.\n",
            "[INFO] AgentBehavior. Step: 8770000. Time Elapsed: 7988.496 s. Mean Reward: 1.683. Std of Reward: 2.462. Training.\n",
            "[INFO] AgentBehavior. Step: 8780000. Time Elapsed: 8053.704 s. Mean Reward: 1.203. Std of Reward: 2.415. Training.\n",
            "[INFO] AgentBehavior. Step: 8790000. Time Elapsed: 8118.329 s. Mean Reward: 1.625. Std of Reward: 2.587. Training.\n",
            "[INFO] AgentBehavior. Step: 8800000. Time Elapsed: 8182.202 s. Mean Reward: 1.600. Std of Reward: 2.396. Training.\n",
            "[INFO] AgentBehavior. Step: 8810000. Time Elapsed: 8246.936 s. Mean Reward: 1.347. Std of Reward: 2.445. Training.\n",
            "[INFO] AgentBehavior. Step: 8820000. Time Elapsed: 8311.923 s. Mean Reward: 1.418. Std of Reward: 2.385. Training.\n",
            "[INFO] AgentBehavior. Step: 8830000. Time Elapsed: 8376.370 s. Mean Reward: 0.967. Std of Reward: 2.387. Training.\n",
            "[INFO] AgentBehavior. Step: 8840000. Time Elapsed: 8438.236 s. Mean Reward: 1.251. Std of Reward: 2.279. Training.\n",
            "[INFO] AgentBehavior. Step: 8850000. Time Elapsed: 8502.520 s. Mean Reward: 1.366. Std of Reward: 2.344. Training.\n",
            "[INFO] AgentBehavior. Step: 8860000. Time Elapsed: 8565.963 s. Mean Reward: 1.816. Std of Reward: 2.323. Training.\n",
            "[INFO] AgentBehavior. Step: 8870000. Time Elapsed: 8630.587 s. Mean Reward: 1.080. Std of Reward: 2.425. Training.\n",
            "[INFO] AgentBehavior. Step: 8880000. Time Elapsed: 8693.496 s. Mean Reward: 1.484. Std of Reward: 2.444. Training.\n",
            "[INFO] AgentBehavior. Step: 8890000. Time Elapsed: 8756.656 s. Mean Reward: 1.334. Std of Reward: 2.371. Training.\n",
            "[INFO] AgentBehavior. Step: 8900000. Time Elapsed: 8821.419 s. Mean Reward: 1.159. Std of Reward: 2.195. Training.\n",
            "[INFO] AgentBehavior. Step: 8910000. Time Elapsed: 8884.436 s. Mean Reward: 1.915. Std of Reward: 2.425. Training.\n",
            "[INFO] AgentBehavior. Step: 8920000. Time Elapsed: 8947.839 s. Mean Reward: 1.758. Std of Reward: 2.385. Training.\n",
            "[INFO] AgentBehavior. Step: 8930000. Time Elapsed: 9011.917 s. Mean Reward: 1.488. Std of Reward: 2.429. Training.\n",
            "[INFO] AgentBehavior. Step: 8940000. Time Elapsed: 9074.433 s. Mean Reward: 1.484. Std of Reward: 2.512. Training.\n",
            "[INFO] AgentBehavior. Step: 8950000. Time Elapsed: 9137.007 s. Mean Reward: 0.963. Std of Reward: 2.256. Training.\n",
            "[INFO] AgentBehavior. Step: 8960000. Time Elapsed: 9202.288 s. Mean Reward: 1.870. Std of Reward: 2.330. Training.\n",
            "[INFO] AgentBehavior. Step: 8970000. Time Elapsed: 9265.056 s. Mean Reward: 1.364. Std of Reward: 2.357. Training.\n",
            "[INFO] AgentBehavior. Step: 8980000. Time Elapsed: 9330.382 s. Mean Reward: 1.242. Std of Reward: 2.479. Training.\n",
            "[INFO] AgentBehavior. Step: 8990000. Time Elapsed: 9393.600 s. Mean Reward: 1.150. Std of Reward: 2.504. Training.\n",
            "[INFO] AgentBehavior. Step: 9000000. Time Elapsed: 9456.359 s. Mean Reward: 1.413. Std of Reward: 2.362. Training.\n",
            "[INFO] Exported results/ColabTest/AgentBehavior/AgentBehavior-8999995.onnx\n",
            "[INFO] AgentBehavior. Step: 9010000. Time Elapsed: 9519.160 s. Mean Reward: 1.284. Std of Reward: 2.339. Training.\n",
            "[INFO] AgentBehavior. Step: 9020000. Time Elapsed: 9581.133 s. Mean Reward: 1.688. Std of Reward: 2.189. Training.\n",
            "[INFO] AgentBehavior. Step: 9030000. Time Elapsed: 9644.155 s. Mean Reward: 1.032. Std of Reward: 2.181. Training.\n",
            "[INFO] AgentBehavior. Step: 9040000. Time Elapsed: 9708.540 s. Mean Reward: 1.342. Std of Reward: 2.281. Training.\n",
            "[INFO] AgentBehavior. Step: 9050000. Time Elapsed: 9770.042 s. Mean Reward: 1.408. Std of Reward: 2.278. Training.\n",
            "[INFO] AgentBehavior. Step: 9060000. Time Elapsed: 9833.317 s. Mean Reward: 1.250. Std of Reward: 2.181. Training.\n",
            "[INFO] AgentBehavior. Step: 9070000. Time Elapsed: 9898.694 s. Mean Reward: 0.949. Std of Reward: 2.289. Training.\n",
            "[INFO] AgentBehavior. Step: 9080000. Time Elapsed: 9961.952 s. Mean Reward: 1.618. Std of Reward: 2.406. Training.\n",
            "[INFO] AgentBehavior. Step: 9090000. Time Elapsed: 10027.571 s. Mean Reward: 1.608. Std of Reward: 2.439. Training.\n",
            "[INFO] AgentBehavior. Step: 9100000. Time Elapsed: 10090.841 s. Mean Reward: 1.379. Std of Reward: 2.356. Training.\n",
            "[INFO] AgentBehavior. Step: 9110000. Time Elapsed: 10152.814 s. Mean Reward: 1.220. Std of Reward: 2.280. Training.\n",
            "[INFO] AgentBehavior. Step: 9120000. Time Elapsed: 10217.745 s. Mean Reward: 1.495. Std of Reward: 2.328. Training.\n",
            "[INFO] AgentBehavior. Step: 9130000. Time Elapsed: 10281.237 s. Mean Reward: 1.467. Std of Reward: 2.463. Training.\n",
            "[INFO] AgentBehavior. Step: 9140000. Time Elapsed: 10345.839 s. Mean Reward: 1.415. Std of Reward: 2.314. Training.\n",
            "[INFO] AgentBehavior. Step: 9150000. Time Elapsed: 10411.155 s. Mean Reward: 1.730. Std of Reward: 2.361. Training.\n",
            "[INFO] AgentBehavior. Step: 9160000. Time Elapsed: 10475.965 s. Mean Reward: 0.996. Std of Reward: 2.186. Training.\n",
            "[INFO] AgentBehavior. Step: 9170000. Time Elapsed: 10539.840 s. Mean Reward: 1.440. Std of Reward: 2.280. Training.\n",
            "[INFO] AgentBehavior. Step: 9180000. Time Elapsed: 10605.352 s. Mean Reward: 1.413. Std of Reward: 2.305. Training.\n",
            "[INFO] AgentBehavior. Step: 9190000. Time Elapsed: 10668.045 s. Mean Reward: 1.749. Std of Reward: 2.351. Training.\n",
            "[INFO] AgentBehavior. Step: 9200000. Time Elapsed: 10733.584 s. Mean Reward: 1.458. Std of Reward: 2.524. Training.\n",
            "[INFO] AgentBehavior. Step: 9210000. Time Elapsed: 10797.948 s. Mean Reward: 1.177. Std of Reward: 2.307. Training.\n",
            "[INFO] AgentBehavior. Step: 9220000. Time Elapsed: 10861.479 s. Mean Reward: 1.783. Std of Reward: 2.453. Training.\n",
            "[INFO] AgentBehavior. Step: 9230000. Time Elapsed: 10926.980 s. Mean Reward: 1.729. Std of Reward: 2.384. Training.\n",
            "[INFO] AgentBehavior. Step: 9240000. Time Elapsed: 10990.861 s. Mean Reward: 1.669. Std of Reward: 2.393. Training.\n",
            "[INFO] AgentBehavior. Step: 9250000. Time Elapsed: 11054.355 s. Mean Reward: 1.460. Std of Reward: 2.517. Training.\n",
            "[INFO] AgentBehavior. Step: 9260000. Time Elapsed: 11119.611 s. Mean Reward: 1.266. Std of Reward: 2.317. Training.\n",
            "[INFO] AgentBehavior. Step: 9270000. Time Elapsed: 11181.475 s. Mean Reward: 1.454. Std of Reward: 2.236. Training.\n",
            "[INFO] AgentBehavior. Step: 9280000. Time Elapsed: 11246.410 s. Mean Reward: 1.523. Std of Reward: 2.457. Training.\n",
            "[INFO] AgentBehavior. Step: 9290000. Time Elapsed: 11309.502 s. Mean Reward: 0.970. Std of Reward: 2.148. Training.\n",
            "[INFO] AgentBehavior. Step: 9300000. Time Elapsed: 11373.785 s. Mean Reward: 1.022. Std of Reward: 2.317. Training.\n",
            "[INFO] AgentBehavior. Step: 9310000. Time Elapsed: 11437.889 s. Mean Reward: 1.255. Std of Reward: 2.399. Training.\n",
            "[INFO] AgentBehavior. Step: 9320000. Time Elapsed: 11501.781 s. Mean Reward: 0.984. Std of Reward: 2.345. Training.\n",
            "[INFO] AgentBehavior. Step: 9330000. Time Elapsed: 11564.598 s. Mean Reward: 1.027. Std of Reward: 2.328. Training.\n",
            "[INFO] AgentBehavior. Step: 9340000. Time Elapsed: 11628.735 s. Mean Reward: 1.098. Std of Reward: 2.384. Training.\n",
            "[INFO] AgentBehavior. Step: 9350000. Time Elapsed: 11691.806 s. Mean Reward: 1.366. Std of Reward: 2.390. Training.\n",
            "[INFO] AgentBehavior. Step: 9360000. Time Elapsed: 11755.752 s. Mean Reward: 1.558. Std of Reward: 2.460. Training.\n",
            "[INFO] AgentBehavior. Step: 9370000. Time Elapsed: 11820.459 s. Mean Reward: 0.957. Std of Reward: 2.377. Training.\n",
            "[INFO] AgentBehavior. Step: 9380000. Time Elapsed: 11883.833 s. Mean Reward: 1.352. Std of Reward: 2.463. Training.\n",
            "[INFO] AgentBehavior. Step: 9390000. Time Elapsed: 11949.144 s. Mean Reward: 0.923. Std of Reward: 2.324. Training.\n",
            "[INFO] AgentBehavior. Step: 9400000. Time Elapsed: 12010.508 s. Mean Reward: 0.858. Std of Reward: 2.314. Training.\n",
            "[INFO] AgentBehavior. Step: 9410000. Time Elapsed: 12073.182 s. Mean Reward: 0.937. Std of Reward: 2.079. Training.\n",
            "[INFO] AgentBehavior. Step: 9420000. Time Elapsed: 12137.764 s. Mean Reward: 0.968. Std of Reward: 2.368. Training.\n",
            "[INFO] AgentBehavior. Step: 9430000. Time Elapsed: 12201.462 s. Mean Reward: 1.308. Std of Reward: 2.347. Training.\n",
            "[INFO] AgentBehavior. Step: 9440000. Time Elapsed: 12264.769 s. Mean Reward: 1.628. Std of Reward: 2.395. Training.\n",
            "[INFO] AgentBehavior. Step: 9450000. Time Elapsed: 12329.786 s. Mean Reward: 1.253. Std of Reward: 2.361. Training.\n",
            "[INFO] AgentBehavior. Step: 9460000. Time Elapsed: 12393.206 s. Mean Reward: 1.046. Std of Reward: 2.282. Training.\n",
            "[INFO] AgentBehavior. Step: 9470000. Time Elapsed: 12455.895 s. Mean Reward: 1.209. Std of Reward: 2.307. Training.\n",
            "[INFO] AgentBehavior. Step: 9480000. Time Elapsed: 12521.584 s. Mean Reward: 1.659. Std of Reward: 2.324. Training.\n",
            "[INFO] AgentBehavior. Step: 9490000. Time Elapsed: 12583.921 s. Mean Reward: 0.761. Std of Reward: 2.237. Training.\n",
            "[INFO] AgentBehavior. Step: 9500000. Time Elapsed: 12649.300 s. Mean Reward: 1.445. Std of Reward: 2.491. Training.\n",
            "[INFO] Exported results/ColabTest/AgentBehavior/AgentBehavior-9499972.onnx\n",
            "[INFO] AgentBehavior. Step: 9510000. Time Elapsed: 12712.249 s. Mean Reward: 1.404. Std of Reward: 2.305. Training.\n",
            "[INFO] AgentBehavior. Step: 9520000. Time Elapsed: 12775.580 s. Mean Reward: 1.063. Std of Reward: 2.276. Training.\n",
            "[INFO] AgentBehavior. Step: 9530000. Time Elapsed: 12840.214 s. Mean Reward: 0.709. Std of Reward: 2.494. Training.\n",
            "[INFO] AgentBehavior. Step: 9540000. Time Elapsed: 12903.458 s. Mean Reward: 1.385. Std of Reward: 2.523. Training.\n",
            "[INFO] AgentBehavior. Step: 9550000. Time Elapsed: 12965.644 s. Mean Reward: 1.125. Std of Reward: 2.553. Training.\n",
            "[INFO] AgentBehavior. Step: 9560000. Time Elapsed: 13030.232 s. Mean Reward: 2.022. Std of Reward: 2.466. Training.\n",
            "[INFO] AgentBehavior. Step: 9570000. Time Elapsed: 13092.185 s. Mean Reward: 1.125. Std of Reward: 2.584. Training.\n",
            "[INFO] AgentBehavior. Step: 9580000. Time Elapsed: 13156.767 s. Mean Reward: 1.478. Std of Reward: 2.315. Training.\n",
            "[INFO] AgentBehavior. Step: 9590000. Time Elapsed: 13219.875 s. Mean Reward: 1.228. Std of Reward: 2.208. Training.\n",
            "[INFO] AgentBehavior. Step: 9600000. Time Elapsed: 13283.135 s. Mean Reward: 1.123. Std of Reward: 2.392. Training.\n",
            "[INFO] AgentBehavior. Step: 9610000. Time Elapsed: 13347.833 s. Mean Reward: 1.361. Std of Reward: 2.190. Training.\n",
            "[INFO] AgentBehavior. Step: 9620000. Time Elapsed: 13409.196 s. Mean Reward: 1.175. Std of Reward: 2.436. Training.\n",
            "[INFO] AgentBehavior. Step: 9630000. Time Elapsed: 13473.253 s. Mean Reward: 1.664. Std of Reward: 2.319. Training.\n",
            "[INFO] AgentBehavior. Step: 9640000. Time Elapsed: 13538.257 s. Mean Reward: 1.447. Std of Reward: 2.303. Training.\n",
            "[INFO] AgentBehavior. Step: 9650000. Time Elapsed: 13601.965 s. Mean Reward: 1.425. Std of Reward: 2.618. Training.\n",
            "[INFO] AgentBehavior. Step: 9660000. Time Elapsed: 13666.764 s. Mean Reward: 1.362. Std of Reward: 2.462. Training.\n",
            "[INFO] AgentBehavior. Step: 9670000. Time Elapsed: 13730.804 s. Mean Reward: 1.755. Std of Reward: 2.497. Training.\n",
            "[INFO] AgentBehavior. Step: 9680000. Time Elapsed: 13792.315 s. Mean Reward: 1.465. Std of Reward: 2.313. Training.\n",
            "[INFO] AgentBehavior. Step: 9690000. Time Elapsed: 13856.639 s. Mean Reward: 1.311. Std of Reward: 2.393. Training.\n",
            "[INFO] AgentBehavior. Step: 9700000. Time Elapsed: 13919.780 s. Mean Reward: 1.281. Std of Reward: 2.437. Training.\n",
            "[INFO] AgentBehavior. Step: 9710000. Time Elapsed: 13983.676 s. Mean Reward: 1.184. Std of Reward: 2.406. Training.\n",
            "[INFO] AgentBehavior. Step: 9720000. Time Elapsed: 14048.706 s. Mean Reward: 1.059. Std of Reward: 2.389. Training.\n",
            "[INFO] AgentBehavior. Step: 9730000. Time Elapsed: 14112.056 s. Mean Reward: 1.235. Std of Reward: 2.246. Training.\n",
            "[INFO] AgentBehavior. Step: 9740000. Time Elapsed: 14176.547 s. Mean Reward: 1.123. Std of Reward: 2.470. Training.\n",
            "[INFO] AgentBehavior. Step: 9750000. Time Elapsed: 14238.527 s. Mean Reward: 1.189. Std of Reward: 2.292. Training.\n",
            "[INFO] AgentBehavior. Step: 9760000. Time Elapsed: 14301.934 s. Mean Reward: 1.616. Std of Reward: 2.266. Training.\n",
            "[INFO] AgentBehavior. Step: 9770000. Time Elapsed: 14367.086 s. Mean Reward: 1.206. Std of Reward: 2.249. Training.\n",
            "[INFO] AgentBehavior. Step: 9780000. Time Elapsed: 14430.345 s. Mean Reward: 0.776. Std of Reward: 2.286. Training.\n",
            "[INFO] AgentBehavior. Step: 9790000. Time Elapsed: 14492.260 s. Mean Reward: 1.328. Std of Reward: 2.266. Training.\n",
            "[INFO] AgentBehavior. Step: 9800000. Time Elapsed: 14556.716 s. Mean Reward: 1.542. Std of Reward: 2.301. Training.\n",
            "[INFO] AgentBehavior. Step: 9810000. Time Elapsed: 14619.076 s. Mean Reward: 1.567. Std of Reward: 2.550. Training.\n",
            "[INFO] AgentBehavior. Step: 9820000. Time Elapsed: 14683.335 s. Mean Reward: 1.238. Std of Reward: 2.399. Training.\n",
            "[INFO] AgentBehavior. Step: 9830000. Time Elapsed: 14746.902 s. Mean Reward: 1.647. Std of Reward: 2.332. Training.\n",
            "[INFO] AgentBehavior. Step: 9840000. Time Elapsed: 14810.910 s. Mean Reward: 1.493. Std of Reward: 2.547. Training.\n",
            "[INFO] AgentBehavior. Step: 9850000. Time Elapsed: 14874.669 s. Mean Reward: 0.941. Std of Reward: 2.397. Training.\n",
            "[INFO] AgentBehavior. Step: 9860000. Time Elapsed: 14935.323 s. Mean Reward: 1.401. Std of Reward: 2.293. Training.\n",
            "[INFO] AgentBehavior. Step: 9870000. Time Elapsed: 14998.002 s. Mean Reward: 1.506. Std of Reward: 2.424. Training.\n",
            "[INFO] AgentBehavior. Step: 9880000. Time Elapsed: 15062.962 s. Mean Reward: 0.845. Std of Reward: 2.268. Training.\n",
            "[INFO] AgentBehavior. Step: 9890000. Time Elapsed: 15125.522 s. Mean Reward: 0.754. Std of Reward: 2.750. Training.\n",
            "[INFO] AgentBehavior. Step: 9900000. Time Elapsed: 15187.019 s. Mean Reward: 1.036. Std of Reward: 2.360. Training.\n",
            "[INFO] AgentBehavior. Step: 9910000. Time Elapsed: 15250.232 s. Mean Reward: 0.944. Std of Reward: 2.300. Training.\n",
            "[INFO] AgentBehavior. Step: 9920000. Time Elapsed: 15312.854 s. Mean Reward: 1.241. Std of Reward: 2.318. Training.\n",
            "[INFO] AgentBehavior. Step: 9930000. Time Elapsed: 15376.408 s. Mean Reward: 1.365. Std of Reward: 2.331. Training.\n",
            "[INFO] AgentBehavior. Step: 9940000. Time Elapsed: 15437.117 s. Mean Reward: 1.148. Std of Reward: 2.385. Training.\n",
            "[INFO] AgentBehavior. Step: 9950000. Time Elapsed: 15498.970 s. Mean Reward: 1.190. Std of Reward: 2.429. Training.\n",
            "[INFO] AgentBehavior. Step: 9960000. Time Elapsed: 15562.601 s. Mean Reward: 1.011. Std of Reward: 2.178. Training.\n",
            "[INFO] AgentBehavior. Step: 9970000. Time Elapsed: 15623.984 s. Mean Reward: 1.029. Std of Reward: 2.367. Training.\n",
            "[INFO] AgentBehavior. Step: 9980000. Time Elapsed: 15684.985 s. Mean Reward: 1.153. Std of Reward: 2.419. Training.\n",
            "[INFO] AgentBehavior. Step: 9990000. Time Elapsed: 15747.990 s. Mean Reward: 1.171. Std of Reward: 2.300. Training.\n",
            "[INFO] AgentBehavior. Step: 10000000. Time Elapsed: 15809.710 s. Mean Reward: 1.647. Std of Reward: 2.423. Training.\n",
            "[INFO] Exported results/ColabTest/AgentBehavior/AgentBehavior-9999996.onnx\n",
            "[INFO] Exported results/ColabTest/AgentBehavior/AgentBehavior-10000060.onnx\n",
            "[INFO] Copied results/ColabTest/AgentBehavior/AgentBehavior-10000060.onnx to results/ColabTest/AgentBehavior.onnx.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/files.zip"
      ],
      "metadata": {
        "id": "5MFYeb4Tbppi"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/files.zip /content/results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MtTYg2c95XR",
        "outputId": "8fd55f84-9bde-4015-e367-b3c2e2ca5fec"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/results/ (stored 0%)\n",
            "  adding: content/results/ColabTest/ (stored 0%)\n",
            "  adding: content/results/ColabTest/run_logs/ (stored 0%)\n",
            "  adding: content/results/ColabTest/run_logs/training_status.json (deflated 82%)\n",
            "  adding: content/results/ColabTest/run_logs/timers.json (deflated 86%)\n",
            "  adding: content/results/ColabTest/run_logs/Player-3.log (deflated 78%)\n",
            "  adding: content/results/ColabTest/run_logs/Player-2.log (deflated 78%)\n",
            "  adding: content/results/ColabTest/run_logs/Player-0.log (deflated 77%)\n",
            "  adding: content/results/ColabTest/run_logs/Player-1.log (deflated 77%)\n",
            "  adding: content/results/ColabTest/AgentBehavior/ (stored 0%)\n",
            "  adding: content/results/ColabTest/AgentBehavior/AgentBehavior-8999995.onnx (deflated 8%)\n",
            "  adding: content/results/ColabTest/AgentBehavior/events.out.tfevents.1710571228.671fed95192d.132194.0 (deflated 96%)\n",
            "  adding: content/results/ColabTest/AgentBehavior/events.out.tfevents.1710592934.671fed95192d.225608.0 (deflated 96%)\n",
            "  adding: content/results/ColabTest/AgentBehavior/AgentBehavior-9499972.onnx (deflated 8%)\n",
            "  adding: content/results/ColabTest/AgentBehavior/AgentBehavior-8999995.pt (deflated 6%)\n",
            "  adding: content/results/ColabTest/AgentBehavior/AgentBehavior-9999996.onnx (deflated 8%)\n",
            "  adding: content/results/ColabTest/AgentBehavior/AgentBehavior-8499962.pt (deflated 6%)\n",
            "  adding: content/results/ColabTest/AgentBehavior/checkpoint.pt (deflated 6%)\n",
            "  adding: content/results/ColabTest/AgentBehavior/AgentBehavior-10000060.onnx (deflated 8%)\n",
            "  adding: content/results/ColabTest/AgentBehavior/AgentBehavior-8499962.onnx (deflated 8%)\n",
            "  adding: content/results/ColabTest/AgentBehavior/AgentBehavior-10000060.pt (deflated 6%)\n",
            "  adding: content/results/ColabTest/AgentBehavior/AgentBehavior-9999996.pt (deflated 6%)\n",
            "  adding: content/results/ColabTest/AgentBehavior/AgentBehavior-9499972.pt (deflated 6%)\n",
            "  adding: content/results/ColabTest/AgentBehavior/AgentBehavior-7900384.pt (deflated 63%)\n",
            "  adding: content/results/ColabTest/AgentBehavior/events.out.tfevents.1710541824.671fed95192d.5577.0 (deflated 96%)\n",
            "  adding: content/results/ColabTest/AgentBehavior.onnx (deflated 8%)\n",
            "  adding: content/results/ColabTest/configuration.yaml (deflated 59%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/files.zip')"
      ],
      "metadata": {
        "id": "BtP9zO1Ob0P1",
        "outputId": "e58d91ca-dd98-41b7-ee8a-8b54833e819a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_547c3564-a3bf-4c33-a263-ec6b90eca529\", \"files.zip\", 35968003)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}