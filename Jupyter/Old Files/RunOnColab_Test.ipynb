{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jydQTw71rlIl",
        "-Tssi5gl7fIh",
        "U0CAJK_t7pbw",
        "rGoYOliVTfxh",
        "Yw6O7m-UTnJp"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyORFpRDQax95RNFO/8iPUd8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elymsyr/MLProject/blob/main/Jupyter/RunOnColab_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Environment"
      ],
      "metadata": {
        "id": "rhZLlCW3iZBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/sample_data"
      ],
      "metadata": {
        "id": "JqHMB_sc6aT-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cuda Downgrade"
      ],
      "metadata": {
        "id": "ylB_bZSvbOdz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxHtkoGOHUV5"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get purge \"cuda*\"\n",
        "!rm -rf /usr/local/cuda*\n",
        "!sudo apt-get autoremove\n",
        "!sudo apt-get autoclean"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "s9UMlY9ULU65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install other import packages\n",
        "!sudo apt-get install g++ freeglut3-dev build-essential libx11-dev \\\n",
        "    libxmu-dev libxi-dev libglu1-mesa libglu1-mesa-dev"
      ],
      "metadata": {
        "id": "HDMy3Zzmucjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin\n",
        "!sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda-repo-ubuntu2204-11-8-local_11.8.0-520.61.05-1_amd64.deb\n",
        "!sudo dpkg -i cuda-repo-ubuntu2204-11-8-local_11.8.0-520.61.05-1_amd64.deb\n",
        "!sudo cp /var/cuda-repo-ubuntu2204-11-8-local/cuda-*-keyring.gpg /usr/share/keyrings/\n",
        "!sudo apt-get update\n",
        "!sudo apt-get -y install cuda-11.8"
      ],
      "metadata": {
        "id": "n3R-hJUAMzp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(1, \"/usr/local/cuda-11.8/lib64\")\n",
        "sys.path.insert(1, \"/usr/local/cuda-11.8/bin\")"
      ],
      "metadata": {
        "id": "k3kD6rVx0GJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo 'export PATH=/usr/local/cuda-11.8/bin:$PATH' >> ~/.bashrc\n",
        "!echo 'export LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc\n",
        "!source ~/.bashrc"
      ],
      "metadata": {
        "id": "JAHb0AkvM3Ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -r /usr/local/cuda-xxx/lib64/libcudnn*"
      ],
      "metadata": {
        "id": "tlbpHORdwAU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo wget https://developer.download.nvidia.com/compute/redist/cudnn/v8.6.0/local_installers/11.8/cudnn-linux-x86_64-8.6.0.163_cuda11-archive.tar.xz\n",
        "!sudo tar -xvf /content/cudnn-linux-x86_64-8.6.0.163_cuda11-archive.tar.xz\n",
        "!sudo mv /content/cudnn-linux-x86_64-8.6.0.163_cuda11-archive cuda\n",
        "\n",
        "# copy the following files into the cuda toolkit directory.\n",
        "!sudo cp -P cuda/include/cudnn.h /usr/local/cuda-11.8/include\n",
        "!sudo cp -P cuda/lib/libcudnn* /usr/local/cuda-11.8/lib64/\n",
        "!sudo chmod a+r /usr/local/cuda-11.8/lib64/libcudnn*\n",
        "\n",
        "# Finally, to verify the installation, check\n",
        "!nvidia-smi\n",
        "!nvcc -V"
      ],
      "metadata": {
        "id": "SatYiSBJppf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "id": "v73JeIt80uUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Check if GPU is available\n",
        "!nvcc --version\n",
        "!nvidia-smi\n",
        "if torch.cuda.is_available():\n",
        "    torch.set_default_device('cuda')\n",
        "    # Get the number of available GPUs\n",
        "    num_gpus = torch.cuda.device_count()\n",
        "    if num_gpus > 0:\n",
        "        print(f\"Number of available GPU devices: {num_gpus}\")\n",
        "        # Iterate over available GPUs and print their index and name\n",
        "        for gpu_index in range(num_gpus):\n",
        "            gpu_name = torch.cuda.get_device_name(gpu_index)\n",
        "            print(f\"GPU {gpu_index}: {gpu_name}\")\n",
        "    else:\n",
        "        print(\"No GPU devices available.\")\n",
        "else:\n",
        "    print(\"GPU is not available.\")"
      ],
      "metadata": {
        "id": "HAPC43s3wMHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /usr/src/cudnn_samples_v8/ $HOME\n",
        "! cd  $HOME/cudnn_samples_v8/mnistCUDNN\n",
        "!make clean && make\n",
        "! ./mnistCUDNN"
      ],
      "metadata": {
        "id": "YqvkFWehZWs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "id": "XUHpwc72aI51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Python Version and Packages"
      ],
      "metadata": {
        "id": "5iOfth00ibKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-KnehGu6Czy",
        "outputId": "ed2d03e2-896b-4736-a224-5ad03bdbfc10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downgrade the python version to v3.9. Install necessary packages. You can find it on [github.com/elymsyr/MLProject](https://github.com/elymsyr/MLProject) (/Docs/requirements.txt)"
      ],
      "metadata": {
        "id": "Qf9eFFFWcwof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update -y\n",
        "!sudo apt-get install python3.9\n",
        "\n",
        "#change alternatives\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 1\n",
        "\n",
        "#check python version\n",
        "!python --version\n",
        "\n",
        "# install pip for new python\n",
        "!sudo apt-get install python3.9-distutils\n",
        "!wget https://bootstrap.pypa.io/get-pip.py\n",
        "!python get-pip.py\n",
        "\n",
        "# install colab's dependencies\n",
        "!python -m pip install ipython tensorflow==2.13.0 tensorboard ipython_genutils ipykernel jupyter_console prompt_toolkit httplib2 astor\n",
        "\n",
        "# link to the old google package\n",
        "!ln -s /usr/local/lib/python3.9/dist-packages/google\n",
        "\n",
        "# Change between versions\n",
        "# !sudo update-alternatives --config python3 <RowNumber>\n",
        "\n",
        "!python --version"
      ],
      "metadata": {
        "id": "wzPbOvSDmOlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install --upgrade pip\n",
        "!pip install --upgrade setuptools pip wheel\n",
        "!pip install mlagents==0.30.0\n",
        "!pip install mlagents-envs==0.30.0"
      ],
      "metadata": {
        "id": "Ke_8hbCjhC8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "id": "38Y3m_gipliD",
        "outputId": "bf9ccc45-28f0-4396-ea95-b00e562c8f85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0\n",
            "    Uninstalling torch-1.11.0:\n",
            "      Successfully uninstalled torch-1.11.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "mlagents 0.30.0 requires torch<=1.11.0,>=1.8.0; platform_system != \"Windows\" and python_version >= \"3.9\", but you have torch 2.2.1+cu118 which is incompatible.\n",
            "tensorflow 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.21.2 which is incompatible.\n",
            "tensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fsspec-2023.4.0 jinja2-3.1.2 mpmath-1.3.0 networkx-3.2.1 nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-8.7.0.84 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.19.3 nvidia-nvtx-cu11-11.8.86 sympy-1.12 torch-2.2.1+cu118 torchaudio-2.2.1+cu118 torchvision-0.17.1+cu118 triton-2.2.0 typing-extensions-4.8.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip3 install torch torchvision torchaudio"
      ],
      "metadata": {
        "id": "1x1v4fG77535"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nvidia-pyindex\n",
        "# !pip install --upgrade nvidia-tensorrt"
      ],
      "metadata": {
        "id": "aLrC1yVnG3Hj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4506882-6ab0-4703-97c0-59167af070a5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-pyindex\n",
            "  Downloading nvidia-pyindex-1.0.9.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: nvidia-pyindex\n",
            "  Building wheel for nvidia-pyindex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-pyindex: filename=nvidia_pyindex-1.0.9-py3-none-any.whl size=8419 sha256=8d5448cd7ee45ef4eb3e686924ad9e4f037fe7a1640373bb7e1fd903cda649d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/63/71/c50214b560fa8c319598c2de3c1616f6d68e1d2c7f17a5e82d\n",
            "Successfully built nvidia-pyindex\n",
            "Installing collected packages: nvidia-pyindex\n",
            "Successfully installed nvidia-pyindex-1.0.9\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install protobuf==3.20.3 install numpy==1.23.5 onnx==1.15.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "id": "bYDF5ZkU8I9g",
        "outputId": "bee32c91-b720-47c1-932d-8f9e4ff59023"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting protobuf==3.20.3\n",
            "  Downloading protobuf-3.20.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (679 bytes)\n",
            "Collecting install\n",
            "  Downloading install-1.3.5-py3-none-any.whl.metadata (925 bytes)\n",
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting onnx==1.15.0\n",
            "  Downloading onnx-1.15.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Downloading protobuf-3.20.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.15.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading install-1.3.5-py3-none-any.whl (3.2 kB)\n",
            "Installing collected packages: protobuf, numpy, install, onnx\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.3\n",
            "    Uninstalling protobuf-4.25.3:\n",
            "      Successfully uninstalled protobuf-4.25.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.2\n",
            "    Uninstalling numpy-1.21.2:\n",
            "      Successfully uninstalled numpy-1.21.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "mlagents 0.30.0 requires torch<=1.11.0,>=1.8.0; platform_system != \"Windows\" and python_version >= \"3.9\", but you have torch 2.2.1+cu118 which is incompatible.\n",
            "mlagents-envs 0.30.0 requires numpy==1.21.2, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed install-1.3.5 numpy-1.23.5 onnx-1.15.0 protobuf-3.20.3\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "89e1b675905c45d09d643ffdd3a393b8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check Environment"
      ],
      "metadata": {
        "id": "xSFmvS2AWmTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Tensorflow-Pytorch GPU"
      ],
      "metadata": {
        "id": "IzJBP6GxdXJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !ln -s /usr/local/lib/python3.9/dist-packages/tensorrt_libs/libnvinfer.so.8 /usr/lib64-nvidia/libnvinfer.so.7\n",
        "# !ln -s /usr/local/lib/python3.9/dist-packages/tensorrt_libs/libnvinfer_plugin.so.8 /usr/lib64-nvidia/libnvinfer_plugin.so.7\n",
        "# !ln -s /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so.12 /usr/lib64-nvidia/libcudart.so.11.0"
      ],
      "metadata": {
        "id": "1vEdkOT0cqoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "# for device in gpu_devices:\n",
        "#     tf.config.experimental.set_memory_growth(device, True)"
      ],
      "metadata": {
        "id": "lZ75KHZH6Mfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.config.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "id": "iuAc13Vc5rYs",
        "outputId": "4ac8e76e-5c67-4a48-eb38-5be155527a84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Old way (deprecated)\n",
        "# torch.set_default_tensor_type(torch.FloatTensor)\n",
        "\n",
        "# New way\n",
        "torch.set_default_dtype(torch.float32)\n",
        "torch.set_default_device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "sH8bNeh_Agj_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Check if GPU is available\n",
        "!nvcc --version\n",
        "!nvidia-smi\n",
        "if torch.cuda.is_available():\n",
        "    torch.set_default_device('cuda')\n",
        "    # Get the number of available GPUs\n",
        "    num_gpus = torch.cuda.device_count()\n",
        "    if num_gpus > 0:\n",
        "        print(f\"Number of available GPU devices: {num_gpus}\")\n",
        "        # Iterate over available GPUs and print their index and name\n",
        "        for gpu_index in range(num_gpus):\n",
        "            gpu_name = torch.cuda.get_device_name(gpu_index)\n",
        "            print(f\"GPU {gpu_index}: {gpu_name}\")\n",
        "    else:\n",
        "        print(\"No GPU devices available.\")\n",
        "else:\n",
        "    print(\"GPU is not available.\")"
      ],
      "metadata": {
        "id": "GnZuyr3ClOQL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8f1583e-2f54-44ff-8914-9b2183b6dc76"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "Fri Mar 15 22:24:00 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8               9W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "Number of available GPU devices: 1\n",
            "GPU 0: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test MLAgents"
      ],
      "metadata": {
        "id": "jydQTw71rlIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mlagents-learn -h"
      ],
      "metadata": {
        "id": "GW5cCfHAiRYW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25ac92bd-4809-425c-800a-f760b3ba4549"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
            "  _C._set_default_tensor_type(t)\n",
            "2024-03-15 22:29:12.497810: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "usage: mlagents-learn [-h] [--env ENV_PATH] [--resume] [--deterministic] [--force]\n",
            "                      [--run-id RUN_ID] [--initialize-from RUN_ID] [--seed SEED] [--inference]\n",
            "                      [--base-port BASE_PORT] [--num-envs NUM_ENVS] [--num-areas NUM_AREAS]\n",
            "                      [--debug] [--env-args ...] [--max-lifetime-restarts MAX_LIFETIME_RESTARTS]\n",
            "                      [--restarts-rate-limit-n RESTARTS_RATE_LIMIT_N]\n",
            "                      [--restarts-rate-limit-period-s RESTARTS_RATE_LIMIT_PERIOD_S] [--torch]\n",
            "                      [--tensorflow] [--results-dir RESULTS_DIR] [--width WIDTH] [--height HEIGHT]\n",
            "                      [--quality-level QUALITY_LEVEL] [--time-scale TIME_SCALE]\n",
            "                      [--target-frame-rate TARGET_FRAME_RATE]\n",
            "                      [--capture-frame-rate CAPTURE_FRAME_RATE] [--no-graphics]\n",
            "                      [--torch-device DEVICE]\n",
            "                      [trainer_config_path]\n",
            "\n",
            "positional arguments:\n",
            "  trainer_config_path\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --env ENV_PATH        Path to the Unity executable to train (default: None)\n",
            "  --resume              Whether to resume training from a checkpoint. Specify a --run-id to use\n",
            "                        this option. If set, the training code loads an already trained model to\n",
            "                        initialize the neural network before resuming training. This option is\n",
            "                        only valid when the models exist, and have the same behavior names as the\n",
            "                        current agents in your scene. (default: False)\n",
            "  --deterministic       Whether to select actions deterministically in policy. `dist.mean` for\n",
            "                        continuous action space, and `dist.argmax` for deterministic action space\n",
            "                        (default: False)\n",
            "  --force               Whether to force-overwrite this run-id's existing summary and model data.\n",
            "                        (Without this flag, attempting to train a model with a run-id that has\n",
            "                        been used before will throw an error. (default: False)\n",
            "  --run-id RUN_ID       The identifier for the training run. This identifier is used to name the\n",
            "                        subdirectories in which the trained model and summary statistics are saved\n",
            "                        as well as the saved model itself. If you use TensorBoard to view the\n",
            "                        training statistics, always set a unique run-id for each training run.\n",
            "                        (The statistics for all runs with the same id are combined as if they were\n",
            "                        produced by a the same session.) (default: ppo)\n",
            "  --initialize-from RUN_ID\n",
            "                        Specify a previously saved run ID from which to initialize the model from.\n",
            "                        This can be used, for instance, to fine-tune an existing model on a new\n",
            "                        environment. Note that the previously saved models must have the same\n",
            "                        behavior parameters as your current environment. (default: None)\n",
            "  --seed SEED           A number to use as a seed for the random number generator used by the\n",
            "                        training code (default: -1)\n",
            "  --inference           Whether to run in Python inference mode (i.e. no training). Use with\n",
            "                        --resume to load a model trained with an existing run ID. (default: False)\n",
            "  --base-port BASE_PORT\n",
            "                        The starting port for environment communication. Each concurrent Unity\n",
            "                        environment instance will get assigned a port sequentially, starting from\n",
            "                        the base-port. Each instance will use the port (base_port + worker_id),\n",
            "                        where the worker_id is sequential IDs given to each instance from 0 to\n",
            "                        (num_envs - 1). Note that when training using the Editor rather than an\n",
            "                        executable, the base port will be ignored. (default: 5005)\n",
            "  --num-envs NUM_ENVS   The number of concurrent Unity environment instances to collect\n",
            "                        experiences from when training (default: 1)\n",
            "  --num-areas NUM_AREAS\n",
            "                        The number of parallel training areas in each Unity environment instance.\n",
            "                        (default: 1)\n",
            "  --debug               Whether to enable debug-level logging for some parts of the code (default:\n",
            "                        False)\n",
            "  --env-args ...        Arguments passed to the Unity executable. Be aware that the standalone\n",
            "                        build will also process these as Unity Command Line Arguments. You should\n",
            "                        choose different argument names if you want to create environment-specific\n",
            "                        arguments. All arguments after this flag will be passed to the executable.\n",
            "                        (default: None)\n",
            "  --max-lifetime-restarts MAX_LIFETIME_RESTARTS\n",
            "                        The max number of times a single Unity executable can crash over its\n",
            "                        lifetime before ml-agents exits. Can be set to -1 if no limit is desired.\n",
            "                        (default: 10)\n",
            "  --restarts-rate-limit-n RESTARTS_RATE_LIMIT_N\n",
            "                        The maximum number of times a single Unity executable can crash over a\n",
            "                        period of time (period set in restarts-rate-limit-period-s). Can be set to\n",
            "                        -1 to not use rate limiting with restarts. (default: 1)\n",
            "  --restarts-rate-limit-period-s RESTARTS_RATE_LIMIT_PERIOD_S\n",
            "                        The period of time --restarts-rate-limit-n applies to. (default: 60)\n",
            "  --torch               (Removed) Use the PyTorch framework. (default: False)\n",
            "  --tensorflow          (Removed) Use the TensorFlow framework. (default: False)\n",
            "  --results-dir RESULTS_DIR\n",
            "                        Results base directory (default: results)\n",
            "\n",
            "Engine Configuration:\n",
            "  --width WIDTH         The width of the executable window of the environment(s) in pixels\n",
            "                        (ignored for editor training). (default: 84)\n",
            "  --height HEIGHT       The height of the executable window of the environment(s) in pixels\n",
            "                        (ignored for editor training) (default: 84)\n",
            "  --quality-level QUALITY_LEVEL\n",
            "                        The quality level of the environment(s). Equivalent to calling\n",
            "                        QualitySettings.SetQualityLevel in Unity. (default: 5)\n",
            "  --time-scale TIME_SCALE\n",
            "                        The time scale of the Unity environment(s). Equivalent to setting\n",
            "                        Time.timeScale in Unity. (default: 20)\n",
            "  --target-frame-rate TARGET_FRAME_RATE\n",
            "                        The target frame rate of the Unity environment(s). Equivalent to setting\n",
            "                        Application.targetFrameRate in Unity. (default: -1)\n",
            "  --capture-frame-rate CAPTURE_FRAME_RATE\n",
            "                        The capture frame rate of the Unity environment(s). Equivalent to setting\n",
            "                        Time.captureFramerate in Unity. (default: 60)\n",
            "  --no-graphics         Whether to run the Unity executable in no-graphics mode (i.e. without\n",
            "                        initializing the graphics driver. Use this only if your agents don't use\n",
            "                        visual observations. (default: False)\n",
            "\n",
            "Torch Configuration:\n",
            "  --torch-device DEVICE\n",
            "                        Settings for the default torch.device used in training, for example,\n",
            "                        \"cpu\", \"cuda\", or \"cuda:0\" (default: None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Build"
      ],
      "metadata": {
        "id": "-Tssi5gl7fIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --branch main https://github.com/elymsyr/MLProject.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEa2Kj4G6WZ-",
        "outputId": "f8884e0c-4bd6-4e19-f934-0141c5059558"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MLProject'...\n",
            "remote: Enumerating objects: 1187, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 1187 (delta 14), reused 12 (delta 12), pack-reused 1170\u001b[K\n",
            "Receiving objects: 100% (1187/1187), 435.89 MiB | 16.16 MiB/s, done.\n",
            "Resolving deltas: 100% (631/631), done.\n",
            "Updating files: 100% (527/527), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the permissions\n",
        "!chmod -R 755 /content/MLProject/Jupyter/b061/b061.x86_64\n",
        "!chmod -R 755 /content/MLProject/Jupyter/b061/UnityPlayer.so\n",
        "!ls -l /content/MLProject/Jupyter/b061"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53mAtnX76kw3",
        "outputId": "9e810923-e3d5-4608-843c-cf6ad0aa4b0b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 112816\n",
            "drwxr-xr-x 3 root root     4096 Mar 15 22:29 b061_BurstDebugInformation_DoNotShip\n",
            "drwxr-xr-x 6 root root     4096 Mar 15 22:29 b061_Data\n",
            "-rw-r--r-- 1 root root    16208 Mar 15 22:29 b061_s.debug\n",
            "-rwxr-xr-x 1 root root    15096 Mar 15 22:29 b061.x86_64\n",
            "-rw-r--r-- 1 root root 67164248 Mar 15 22:29 UnityPlayer_s.debug\n",
            "-rwxr-xr-x 1 root root 48314296 Mar 15 22:29 UnityPlayer.so\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create config.yaml"
      ],
      "metadata": {
        "id": "U0CAJK_t7pbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config.yaml\n",
        "\n",
        "default_settings: null\n",
        "behaviors:\n",
        "  AgentBehavior:\n",
        "    trainer_type: ppo\n",
        "    hyperparameters:\n",
        "      batch_size: 1024\n",
        "      buffer_size: 10240\n",
        "      learning_rate: 0.0003\n",
        "      beta: 0.005\n",
        "      epsilon: 0.2\n",
        "      lambd: 0.95\n",
        "      num_epoch: 3\n",
        "      shared_critic: false\n",
        "      learning_rate_schedule: linear\n",
        "      beta_schedule: linear\n",
        "      epsilon_schedule: linear\n",
        "    network_settings:\n",
        "      normalize: false\n",
        "      hidden_units: 256\n",
        "      num_layers: 3\n",
        "      vis_encode_type: simple\n",
        "      memory: null\n",
        "      goal_conditioning_type: hyper\n",
        "      deterministic: false\n",
        "    reward_signals:\n",
        "      extrinsic:\n",
        "        gamma: 0.99\n",
        "        strength: 1.0\n",
        "        network_settings:\n",
        "          normalize: false\n",
        "          hidden_units: 256\n",
        "          num_layers: 3\n",
        "          vis_encode_type: simple\n",
        "          memory: null\n",
        "          goal_conditioning_type: hyper\n",
        "          deterministic: false\n",
        "    init_path: null\n",
        "    keep_checkpoints: 5\n",
        "    checkpoint_interval: 500000\n",
        "    max_steps: 10000000\n",
        "    time_horizon: 64\n",
        "    summary_freq: 10000\n",
        "    threaded: false\n",
        "    self_play: null\n",
        "    behavioral_cloning: null\n",
        "env_settings:\n",
        "  env_path: /content/MLProject/Jupyter/b061/b061.x86_64\n",
        "  env_args: null\n",
        "  base_port: 5004\n",
        "  num_envs: 4\n",
        "  num_areas: 1\n",
        "  seed: -1\n",
        "  max_lifetime_restarts: 10\n",
        "  restarts_rate_limit_n: 1\n",
        "  restarts_rate_limit_period_s: 60\n",
        "engine_settings:\n",
        "  width: 84\n",
        "  height: 84\n",
        "  quality_level: 5\n",
        "  time_scale: 20.0\n",
        "  target_frame_rate: -1\n",
        "  capture_frame_rate: 60\n",
        "  no_graphics: true\n",
        "environment_parameters: null\n",
        "checkpoint_settings:\n",
        "  run_id: ColabTest\n",
        "  initialize_from:\n",
        "  load_model: false\n",
        "  resume: false\n",
        "  force: false\n",
        "  train_model: false\n",
        "  inference: false\n",
        "  results_dir: results\n",
        "torch_settings:\n",
        "  device: cuda\n",
        "debug: false"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyVmeCgc6sUG",
        "outputId": "67fbee5c-c196-4b37-befa-72e9811c1cc7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing config.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tenserboard and Train Process"
      ],
      "metadata": {
        "id": "9IL5Pm_s-O2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorboard"
      ],
      "metadata": {
        "id": "rGoYOliVTfxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir results\n",
        "%reload_ext tensorboard"
      ],
      "metadata": {
        "id": "WwITCL-w-RNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Delete previous trains"
      ],
      "metadata": {
        "id": "Yw6O7m-UTnJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/results # if necessary"
      ],
      "metadata": {
        "id": "u6VC2WGfRyR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "Lj7Q4BsrTkkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mlagents-learn /content/config.yaml --env /content/MLProject/Jupyter/b061/b061.x86_64"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dZ7p5Pw7Yey",
        "outputId": "ea98b7c0-0fe9-41d2-d1b1-59d581e59a65"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
            "  _C._set_default_tensor_type(t)\n",
            "2024-03-15 22:30:21.445540: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "            ┐  ╖\n",
            "        ╓╖╬│╡  ││╬╖╖\n",
            "    ╓╖╬│││││┘  ╬│││││╬╖\n",
            " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
            " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
            " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
            " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
            " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
            " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
            " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
            "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
            "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
            "          ╙╬╬╬╣╣╣╜\n",
            "             ╙\n",
            "        \n",
            " Version information:\n",
            "  ml-agents: 0.30.0,\n",
            "  ml-agents-envs: 0.30.0,\n",
            "  Communicator API: 1.5.0,\n",
            "  PyTorch: 2.2.1+cu118\n",
            "[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0\n",
            "[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0\n",
            "[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0\n",
            "[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0\n",
            "[INFO] Connected new brain: AgentBehavior?team=0\n",
            "[INFO] Connected new brain: AgentBehavior?team=0\n",
            "[INFO] Connected new brain: AgentBehavior?team=0\n",
            "[INFO] Connected new brain: AgentBehavior?team=0\n",
            "[INFO] Hyperparameters for behavior name AgentBehavior: \n",
            "\ttrainer_type:\tppo\n",
            "\thyperparameters:\t\n",
            "\t  batch_size:\t1024\n",
            "\t  buffer_size:\t10240\n",
            "\t  learning_rate:\t0.0003\n",
            "\t  beta:\t0.005\n",
            "\t  epsilon:\t0.2\n",
            "\t  lambd:\t0.95\n",
            "\t  num_epoch:\t3\n",
            "\t  shared_critic:\tFalse\n",
            "\t  learning_rate_schedule:\tlinear\n",
            "\t  beta_schedule:\tlinear\n",
            "\t  epsilon_schedule:\tlinear\n",
            "\tnetwork_settings:\t\n",
            "\t  normalize:\tFalse\n",
            "\t  hidden_units:\t256\n",
            "\t  num_layers:\t3\n",
            "\t  vis_encode_type:\tsimple\n",
            "\t  memory:\tNone\n",
            "\t  goal_conditioning_type:\thyper\n",
            "\t  deterministic:\tFalse\n",
            "\treward_signals:\t\n",
            "\t  extrinsic:\t\n",
            "\t    gamma:\t0.99\n",
            "\t    strength:\t1.0\n",
            "\t    network_settings:\t\n",
            "\t      normalize:\tFalse\n",
            "\t      hidden_units:\t256\n",
            "\t      num_layers:\t3\n",
            "\t      vis_encode_type:\tsimple\n",
            "\t      memory:\tNone\n",
            "\t      goal_conditioning_type:\thyper\n",
            "\t      deterministic:\tFalse\n",
            "\tinit_path:\tNone\n",
            "\tkeep_checkpoints:\t5\n",
            "\tcheckpoint_interval:\t500000\n",
            "\tmax_steps:\t10000000\n",
            "\ttime_horizon:\t64\n",
            "\tsummary_freq:\t10000\n",
            "\tthreaded:\tFalse\n",
            "\tself_play:\tNone\n",
            "\tbehavioral_cloning:\tNone\n",
            "[INFO] AgentBehavior. Step: 10000. Time Elapsed: 68.420 s. Mean Reward: -1.771. Std of Reward: 2.522. Training.\n",
            "[INFO] AgentBehavior. Step: 20000. Time Elapsed: 131.750 s. Mean Reward: -1.917. Std of Reward: 2.745. Training.\n",
            "[INFO] AgentBehavior. Step: 30000. Time Elapsed: 196.904 s. Mean Reward: -1.554. Std of Reward: 2.276. Training.\n",
            "[INFO] AgentBehavior. Step: 40000. Time Elapsed: 258.585 s. Mean Reward: -1.706. Std of Reward: 2.388. Training.\n",
            "[INFO] AgentBehavior. Step: 50000. Time Elapsed: 323.884 s. Mean Reward: -2.073. Std of Reward: 2.384. Training.\n",
            "[INFO] AgentBehavior. Step: 60000. Time Elapsed: 387.138 s. Mean Reward: -1.629. Std of Reward: 2.815. Training.\n",
            "[INFO] AgentBehavior. Step: 70000. Time Elapsed: 450.963 s. Mean Reward: -1.651. Std of Reward: 3.139. Training.\n",
            "[INFO] AgentBehavior. Step: 80000. Time Elapsed: 513.251 s. Mean Reward: -1.736. Std of Reward: 2.443. Training.\n",
            "[INFO] AgentBehavior. Step: 90000. Time Elapsed: 577.871 s. Mean Reward: -1.584. Std of Reward: 2.914. Training.\n",
            "[INFO] AgentBehavior. Step: 100000. Time Elapsed: 641.647 s. Mean Reward: -1.436. Std of Reward: 2.453. Training.\n",
            "[INFO] AgentBehavior. Step: 110000. Time Elapsed: 707.366 s. Mean Reward: -1.644. Std of Reward: 2.604. Training.\n",
            "[INFO] AgentBehavior. Step: 120000. Time Elapsed: 769.453 s. Mean Reward: -1.492. Std of Reward: 2.450. Training.\n",
            "[INFO] AgentBehavior. Step: 130000. Time Elapsed: 834.637 s. Mean Reward: -1.505. Std of Reward: 2.528. Training.\n",
            "[INFO] AgentBehavior. Step: 140000. Time Elapsed: 898.111 s. Mean Reward: -2.117. Std of Reward: 2.545. Training.\n",
            "[INFO] AgentBehavior. Step: 150000. Time Elapsed: 964.448 s. Mean Reward: -2.051. Std of Reward: 2.556. Training.\n",
            "[INFO] AgentBehavior. Step: 160000. Time Elapsed: 1028.939 s. Mean Reward: -1.406. Std of Reward: 2.656. Training.\n",
            "[INFO] AgentBehavior. Step: 170000. Time Elapsed: 1095.027 s. Mean Reward: -1.699. Std of Reward: 2.399. Training.\n",
            "[INFO] AgentBehavior. Step: 180000. Time Elapsed: 1156.869 s. Mean Reward: -1.210. Std of Reward: 1.857. Training.\n",
            "[INFO] AgentBehavior. Step: 190000. Time Elapsed: 1221.501 s. Mean Reward: -1.135. Std of Reward: 2.594. Training.\n",
            "[INFO] AgentBehavior. Step: 200000. Time Elapsed: 1284.646 s. Mean Reward: -1.272. Std of Reward: 2.626. Training.\n",
            "[INFO] AgentBehavior. Step: 210000. Time Elapsed: 1349.501 s. Mean Reward: -1.250. Std of Reward: 2.103. Training.\n",
            "[INFO] AgentBehavior. Step: 220000. Time Elapsed: 1413.927 s. Mean Reward: -1.319. Std of Reward: 2.005. Training.\n",
            "[INFO] AgentBehavior. Step: 230000. Time Elapsed: 1477.515 s. Mean Reward: -1.291. Std of Reward: 2.137. Training.\n",
            "[INFO] AgentBehavior. Step: 240000. Time Elapsed: 1541.577 s. Mean Reward: -1.443. Std of Reward: 1.995. Training.\n",
            "[INFO] AgentBehavior. Step: 250000. Time Elapsed: 1604.910 s. Mean Reward: -1.294. Std of Reward: 2.069. Training.\n",
            "[INFO] AgentBehavior. Step: 260000. Time Elapsed: 1669.545 s. Mean Reward: -1.408. Std of Reward: 2.076. Training.\n",
            "[INFO] AgentBehavior. Step: 270000. Time Elapsed: 1732.633 s. Mean Reward: -1.130. Std of Reward: 2.127. Training.\n",
            "[INFO] AgentBehavior. Step: 280000. Time Elapsed: 1797.494 s. Mean Reward: -1.020. Std of Reward: 2.161. Training.\n",
            "[INFO] AgentBehavior. Step: 290000. Time Elapsed: 1859.865 s. Mean Reward: -1.124. Std of Reward: 2.048. Training.\n",
            "[INFO] AgentBehavior. Step: 300000. Time Elapsed: 1925.325 s. Mean Reward: -1.198. Std of Reward: 2.451. Training.\n",
            "[INFO] AgentBehavior. Step: 310000. Time Elapsed: 1988.372 s. Mean Reward: -0.588. Std of Reward: 2.390. Training.\n",
            "[INFO] AgentBehavior. Step: 320000. Time Elapsed: 2053.133 s. Mean Reward: -0.753. Std of Reward: 1.947. Training.\n",
            "[INFO] AgentBehavior. Step: 330000. Time Elapsed: 2116.107 s. Mean Reward: -1.317. Std of Reward: 2.095. Training.\n",
            "[INFO] AgentBehavior. Step: 340000. Time Elapsed: 2181.516 s. Mean Reward: -0.977. Std of Reward: 2.202. Training.\n",
            "[INFO] AgentBehavior. Step: 350000. Time Elapsed: 2245.215 s. Mean Reward: -0.700. Std of Reward: 1.937. Training.\n",
            "[INFO] AgentBehavior. Step: 360000. Time Elapsed: 2311.941 s. Mean Reward: -0.604. Std of Reward: 2.349. Training.\n",
            "[INFO] AgentBehavior. Step: 370000. Time Elapsed: 2376.755 s. Mean Reward: -0.715. Std of Reward: 1.992. Training.\n",
            "[INFO] AgentBehavior. Step: 380000. Time Elapsed: 2437.765 s. Mean Reward: -0.325. Std of Reward: 2.090. Training.\n",
            "[INFO] AgentBehavior. Step: 390000. Time Elapsed: 2501.289 s. Mean Reward: -0.873. Std of Reward: 2.438. Training.\n",
            "[INFO] AgentBehavior. Step: 400000. Time Elapsed: 2565.776 s. Mean Reward: -0.548. Std of Reward: 1.945. Training.\n",
            "[INFO] AgentBehavior. Step: 410000. Time Elapsed: 2630.842 s. Mean Reward: -0.789. Std of Reward: 2.046. Training.\n",
            "[INFO] AgentBehavior. Step: 420000. Time Elapsed: 2694.257 s. Mean Reward: -0.558. Std of Reward: 1.469. Training.\n",
            "[INFO] AgentBehavior. Step: 430000. Time Elapsed: 2758.544 s. Mean Reward: -0.632. Std of Reward: 1.892. Training.\n",
            "[INFO] AgentBehavior. Step: 440000. Time Elapsed: 2820.424 s. Mean Reward: -0.476. Std of Reward: 1.724. Training.\n",
            "[INFO] AgentBehavior. Step: 450000. Time Elapsed: 2885.349 s. Mean Reward: -0.707. Std of Reward: 1.970. Training.\n",
            "[INFO] AgentBehavior. Step: 460000. Time Elapsed: 2949.512 s. Mean Reward: -0.890. Std of Reward: 2.303. Training.\n",
            "[INFO] AgentBehavior. Step: 470000. Time Elapsed: 3015.446 s. Mean Reward: -0.965. Std of Reward: 2.146. Training.\n",
            "[INFO] AgentBehavior. Step: 480000. Time Elapsed: 3082.946 s. Mean Reward: -0.532. Std of Reward: 1.678. Training.\n",
            "[INFO] AgentBehavior. Step: 490000. Time Elapsed: 3147.738 s. Mean Reward: -0.466. Std of Reward: 1.612. Training.\n",
            "[INFO] AgentBehavior. Step: 500000. Time Elapsed: 3211.312 s. Mean Reward: -0.547. Std of Reward: 1.934. Training.\n",
            "[INFO] Exported results/ColabTest/AgentBehavior/AgentBehavior-499959.onnx\n",
            "[INFO] AgentBehavior. Step: 510000. Time Elapsed: 3274.928 s. Mean Reward: -0.355. Std of Reward: 2.340. Training.\n",
            "[INFO] AgentBehavior. Step: 520000. Time Elapsed: 3339.835 s. Mean Reward: -0.348. Std of Reward: 1.550. Training.\n",
            "[INFO] AgentBehavior. Step: 530000. Time Elapsed: 3402.755 s. Mean Reward: -0.625. Std of Reward: 2.000. Training.\n",
            "[INFO] AgentBehavior. Step: 540000. Time Elapsed: 3466.567 s. Mean Reward: -0.344. Std of Reward: 1.602. Training.\n",
            "[INFO] AgentBehavior. Step: 550000. Time Elapsed: 3527.804 s. Mean Reward: -0.468. Std of Reward: 1.998. Training.\n",
            "[INFO] AgentBehavior. Step: 560000. Time Elapsed: 3592.699 s. Mean Reward: -0.524. Std of Reward: 2.344. Training.\n",
            "[INFO] AgentBehavior. Step: 570000. Time Elapsed: 3657.912 s. Mean Reward: -0.777. Std of Reward: 1.749. Training.\n",
            "[INFO] AgentBehavior. Step: 580000. Time Elapsed: 3720.833 s. Mean Reward: -0.163. Std of Reward: 2.113. Training.\n",
            "[INFO] AgentBehavior. Step: 590000. Time Elapsed: 3784.837 s. Mean Reward: -0.431. Std of Reward: 1.831. Training.\n",
            "[INFO] AgentBehavior. Step: 600000. Time Elapsed: 3848.714 s. Mean Reward: -0.579. Std of Reward: 1.980. Training.\n",
            "[INFO] AgentBehavior. Step: 610000. Time Elapsed: 3914.661 s. Mean Reward: -0.497. Std of Reward: 1.664. Training.\n",
            "[INFO] AgentBehavior. Step: 620000. Time Elapsed: 3979.211 s. Mean Reward: 0.043. Std of Reward: 1.919. Training.\n",
            "[INFO] AgentBehavior. Step: 630000. Time Elapsed: 4044.802 s. Mean Reward: -0.529. Std of Reward: 2.309. Training.\n",
            "[INFO] AgentBehavior. Step: 640000. Time Elapsed: 4109.804 s. Mean Reward: -0.307. Std of Reward: 2.316. Training.\n",
            "[INFO] AgentBehavior. Step: 650000. Time Elapsed: 4172.182 s. Mean Reward: -0.285. Std of Reward: 1.796. Training.\n",
            "[INFO] AgentBehavior. Step: 660000. Time Elapsed: 4238.087 s. Mean Reward: -0.550. Std of Reward: 1.862. Training.\n",
            "[INFO] AgentBehavior. Step: 670000. Time Elapsed: 4302.612 s. Mean Reward: -0.358. Std of Reward: 1.950. Training.\n",
            "[INFO] AgentBehavior. Step: 680000. Time Elapsed: 4368.194 s. Mean Reward: -0.461. Std of Reward: 1.879. Training.\n",
            "[INFO] AgentBehavior. Step: 690000. Time Elapsed: 4431.803 s. Mean Reward: -0.343. Std of Reward: 1.848. Training.\n",
            "[INFO] AgentBehavior. Step: 700000. Time Elapsed: 4496.993 s. Mean Reward: -0.203. Std of Reward: 2.062. Training.\n",
            "[INFO] AgentBehavior. Step: 710000. Time Elapsed: 4561.442 s. Mean Reward: 0.069. Std of Reward: 1.870. Training.\n",
            "[INFO] AgentBehavior. Step: 720000. Time Elapsed: 4624.831 s. Mean Reward: -0.249. Std of Reward: 2.365. Training.\n",
            "[INFO] AgentBehavior. Step: 730000. Time Elapsed: 4689.529 s. Mean Reward: -0.009. Std of Reward: 2.595. Training.\n",
            "[INFO] AgentBehavior. Step: 740000. Time Elapsed: 4752.259 s. Mean Reward: 0.094. Std of Reward: 2.207. Training.\n",
            "[INFO] AgentBehavior. Step: 750000. Time Elapsed: 4817.724 s. Mean Reward: -0.098. Std of Reward: 2.285. Training.\n",
            "[INFO] AgentBehavior. Step: 760000. Time Elapsed: 4877.136 s. Mean Reward: -0.371. Std of Reward: 2.189. Training.\n",
            "[INFO] AgentBehavior. Step: 770000. Time Elapsed: 4942.334 s. Mean Reward: -0.178. Std of Reward: 2.416. Training.\n",
            "[INFO] AgentBehavior. Step: 780000. Time Elapsed: 5005.538 s. Mean Reward: -0.205. Std of Reward: 2.259. Training.\n",
            "[INFO] AgentBehavior. Step: 790000. Time Elapsed: 5070.046 s. Mean Reward: -0.354. Std of Reward: 1.742. Training.\n",
            "[INFO] AgentBehavior. Step: 800000. Time Elapsed: 5134.875 s. Mean Reward: -0.027. Std of Reward: 2.036. Training.\n",
            "[INFO] AgentBehavior. Step: 810000. Time Elapsed: 5198.374 s. Mean Reward: -0.198. Std of Reward: 1.862. Training.\n",
            "[INFO] AgentBehavior. Step: 820000. Time Elapsed: 5262.887 s. Mean Reward: -0.035. Std of Reward: 2.186. Training.\n",
            "[INFO] AgentBehavior. Step: 830000. Time Elapsed: 5325.129 s. Mean Reward: -0.240. Std of Reward: 1.713. Training.\n",
            "[INFO] AgentBehavior. Step: 840000. Time Elapsed: 5390.306 s. Mean Reward: -0.183. Std of Reward: 1.961. Training.\n",
            "[INFO] AgentBehavior. Step: 850000. Time Elapsed: 5455.487 s. Mean Reward: -0.063. Std of Reward: 2.219. Training.\n",
            "[INFO] AgentBehavior. Step: 860000. Time Elapsed: 5517.995 s. Mean Reward: 0.040. Std of Reward: 2.007. Training.\n",
            "[INFO] AgentBehavior. Step: 870000. Time Elapsed: 5582.539 s. Mean Reward: 0.371. Std of Reward: 2.170. Training.\n",
            "[INFO] AgentBehavior. Step: 880000. Time Elapsed: 5644.967 s. Mean Reward: -0.380. Std of Reward: 1.636. Training.\n",
            "[INFO] AgentBehavior. Step: 890000. Time Elapsed: 5709.427 s. Mean Reward: 0.050. Std of Reward: 1.736. Training.\n",
            "[INFO] AgentBehavior. Step: 900000. Time Elapsed: 5772.045 s. Mean Reward: 0.130. Std of Reward: 2.014. Training.\n",
            "[INFO] AgentBehavior. Step: 910000. Time Elapsed: 5836.907 s. Mean Reward: 0.026. Std of Reward: 2.116. Training.\n",
            "[INFO] AgentBehavior. Step: 920000. Time Elapsed: 5901.838 s. Mean Reward: 0.108. Std of Reward: 1.899. Training.\n",
            "[INFO] AgentBehavior. Step: 930000. Time Elapsed: 5963.490 s. Mean Reward: 0.044. Std of Reward: 1.868. Training.\n",
            "[INFO] AgentBehavior. Step: 940000. Time Elapsed: 6028.021 s. Mean Reward: 0.210. Std of Reward: 1.828. Training.\n",
            "[INFO] AgentBehavior. Step: 950000. Time Elapsed: 6090.497 s. Mean Reward: 0.080. Std of Reward: 2.073. Training.\n",
            "[INFO] AgentBehavior. Step: 960000. Time Elapsed: 6155.254 s. Mean Reward: 0.239. Std of Reward: 2.115. Training.\n",
            "[INFO] AgentBehavior. Step: 970000. Time Elapsed: 6219.214 s. Mean Reward: -0.540. Std of Reward: 1.822. Training.\n",
            "[INFO] AgentBehavior. Step: 980000. Time Elapsed: 6283.883 s. Mean Reward: -0.080. Std of Reward: 1.547. Training.\n",
            "[INFO] AgentBehavior. Step: 990000. Time Elapsed: 6348.540 s. Mean Reward: -0.183. Std of Reward: 1.776. Training.\n",
            "[INFO] AgentBehavior. Step: 1000000. Time Elapsed: 6411.744 s. Mean Reward: 0.174. Std of Reward: 1.879. Training.\n",
            "[INFO] Exported results/ColabTest/AgentBehavior/AgentBehavior-999996.onnx\n",
            "[INFO] AgentBehavior. Step: 1010000. Time Elapsed: 6475.979 s. Mean Reward: -0.343. Std of Reward: 1.568. Training.\n",
            "[INFO] AgentBehavior. Step: 1020000. Time Elapsed: 6538.440 s. Mean Reward: 0.009. Std of Reward: 1.870. Training.\n",
            "[INFO] AgentBehavior. Step: 1030000. Time Elapsed: 6602.492 s. Mean Reward: -0.126. Std of Reward: 1.767. Training.\n",
            "[INFO] AgentBehavior. Step: 1040000. Time Elapsed: 6666.462 s. Mean Reward: 0.171. Std of Reward: 1.965. Training.\n",
            "[INFO] AgentBehavior. Step: 1050000. Time Elapsed: 6730.260 s. Mean Reward: -0.180. Std of Reward: 1.818. Training.\n",
            "[INFO] AgentBehavior. Step: 1060000. Time Elapsed: 6793.661 s. Mean Reward: -0.023. Std of Reward: 1.793. Training.\n",
            "[INFO] AgentBehavior. Step: 1070000. Time Elapsed: 6855.626 s. Mean Reward: 0.138. Std of Reward: 1.905. Training.\n",
            "[INFO] AgentBehavior. Step: 1080000. Time Elapsed: 6920.412 s. Mean Reward: -0.090. Std of Reward: 1.541. Training.\n",
            "[INFO] AgentBehavior. Step: 1090000. Time Elapsed: 6982.877 s. Mean Reward: 0.131. Std of Reward: 1.629. Training.\n",
            "[INFO] AgentBehavior. Step: 1100000. Time Elapsed: 7045.693 s. Mean Reward: 0.682. Std of Reward: 2.202. Training.\n",
            "[INFO] AgentBehavior. Step: 1110000. Time Elapsed: 7109.625 s. Mean Reward: 0.396. Std of Reward: 1.880. Training.\n",
            "[INFO] AgentBehavior. Step: 1120000. Time Elapsed: 7173.172 s. Mean Reward: 0.459. Std of Reward: 2.033. Training.\n",
            "[INFO] AgentBehavior. Step: 1130000. Time Elapsed: 7234.487 s. Mean Reward: 0.309. Std of Reward: 2.082. Training.\n",
            "[INFO] AgentBehavior. Step: 1140000. Time Elapsed: 7298.516 s. Mean Reward: -0.104. Std of Reward: 1.723. Training.\n",
            "[INFO] AgentBehavior. Step: 1150000. Time Elapsed: 7363.190 s. Mean Reward: -0.111. Std of Reward: 2.026. Training.\n",
            "[INFO] AgentBehavior. Step: 1160000. Time Elapsed: 7425.415 s. Mean Reward: -0.314. Std of Reward: 1.480. Training.\n",
            "[INFO] AgentBehavior. Step: 1170000. Time Elapsed: 7488.186 s. Mean Reward: -0.405. Std of Reward: 1.525. Training.\n",
            "[INFO] AgentBehavior. Step: 1180000. Time Elapsed: 7551.966 s. Mean Reward: 0.340. Std of Reward: 2.200. Training.\n",
            "[INFO] AgentBehavior. Step: 1190000. Time Elapsed: 7615.742 s. Mean Reward: 0.056. Std of Reward: 2.076. Training.\n",
            "[INFO] AgentBehavior. Step: 1200000. Time Elapsed: 7679.245 s. Mean Reward: 0.370. Std of Reward: 2.131. Training.\n",
            "[INFO] AgentBehavior. Step: 1210000. Time Elapsed: 7740.881 s. Mean Reward: 0.153. Std of Reward: 2.113. Training.\n",
            "[INFO] AgentBehavior. Step: 1220000. Time Elapsed: 7805.108 s. Mean Reward: -0.042. Std of Reward: 2.210. Training.\n",
            "[INFO] AgentBehavior. Step: 1230000. Time Elapsed: 7867.945 s. Mean Reward: -0.241. Std of Reward: 1.907. Training.\n",
            "[INFO] AgentBehavior. Step: 1240000. Time Elapsed: 7930.643 s. Mean Reward: 0.433. Std of Reward: 1.911. Training.\n",
            "[INFO] AgentBehavior. Step: 1250000. Time Elapsed: 7994.385 s. Mean Reward: -0.106. Std of Reward: 1.691. Training.\n",
            "[INFO] AgentBehavior. Step: 1260000. Time Elapsed: 8055.967 s. Mean Reward: 0.158. Std of Reward: 1.999. Training.\n",
            "[INFO] AgentBehavior. Step: 1270000. Time Elapsed: 8120.345 s. Mean Reward: 0.071. Std of Reward: 1.915. Training.\n",
            "[INFO] AgentBehavior. Step: 1280000. Time Elapsed: 8182.703 s. Mean Reward: 0.068. Std of Reward: 2.119. Training.\n",
            "[INFO] AgentBehavior. Step: 1290000. Time Elapsed: 8246.177 s. Mean Reward: 0.001. Std of Reward: 1.975. Training.\n",
            "[INFO] AgentBehavior. Step: 1300000. Time Elapsed: 8309.821 s. Mean Reward: -0.209. Std of Reward: 2.153. Training.\n",
            "[INFO] AgentBehavior. Step: 1310000. Time Elapsed: 8371.971 s. Mean Reward: 0.269. Std of Reward: 2.177. Training.\n",
            "[INFO] AgentBehavior. Step: 1320000. Time Elapsed: 8436.174 s. Mean Reward: -0.033. Std of Reward: 1.739. Training.\n",
            "[INFO] AgentBehavior. Step: 1330000. Time Elapsed: 8497.631 s. Mean Reward: -0.332. Std of Reward: 2.026. Training.\n",
            "[INFO] AgentBehavior. Step: 1340000. Time Elapsed: 8561.618 s. Mean Reward: 0.462. Std of Reward: 2.217. Training.\n",
            "[INFO] AgentBehavior. Step: 1350000. Time Elapsed: 8625.645 s. Mean Reward: 0.213. Std of Reward: 2.216. Training.\n",
            "[INFO] AgentBehavior. Step: 1360000. Time Elapsed: 8687.850 s. Mean Reward: 0.027. Std of Reward: 1.942. Training.\n",
            "[INFO] AgentBehavior. Step: 1370000. Time Elapsed: 8750.976 s. Mean Reward: -0.448. Std of Reward: 1.500. Training.\n",
            "[INFO] AgentBehavior. Step: 1380000. Time Elapsed: 8815.152 s. Mean Reward: 0.326. Std of Reward: 1.943. Training.\n",
            "[INFO] AgentBehavior. Step: 1390000. Time Elapsed: 8882.125 s. Mean Reward: 0.213. Std of Reward: 1.943. Training.\n",
            "[INFO] AgentBehavior. Step: 1400000. Time Elapsed: 8947.637 s. Mean Reward: 0.109. Std of Reward: 2.352. Training.\n",
            "[INFO] AgentBehavior. Step: 1410000. Time Elapsed: 9010.719 s. Mean Reward: 0.385. Std of Reward: 2.334. Training.\n",
            "[INFO] AgentBehavior. Step: 1420000. Time Elapsed: 9074.547 s. Mean Reward: 0.565. Std of Reward: 2.144. Training.\n",
            "[INFO] AgentBehavior. Step: 1430000. Time Elapsed: 9138.522 s. Mean Reward: 0.355. Std of Reward: 2.113. Training.\n",
            "[INFO] AgentBehavior. Step: 1440000. Time Elapsed: 9205.070 s. Mean Reward: 0.159. Std of Reward: 2.041. Training.\n",
            "[INFO] AgentBehavior. Step: 1450000. Time Elapsed: 9269.715 s. Mean Reward: 0.402. Std of Reward: 2.089. Training.\n",
            "[INFO] AgentBehavior. Step: 1460000. Time Elapsed: 9334.160 s. Mean Reward: 0.118. Std of Reward: 2.580. Training.\n",
            "[INFO] AgentBehavior. Step: 1470000. Time Elapsed: 9400.623 s. Mean Reward: 0.539. Std of Reward: 2.104. Training.\n",
            "[INFO] AgentBehavior. Step: 1480000. Time Elapsed: 9464.120 s. Mean Reward: 0.231. Std of Reward: 2.125. Training.\n",
            "[INFO] AgentBehavior. Step: 1490000. Time Elapsed: 9527.167 s. Mean Reward: 0.089. Std of Reward: 2.064. Training.\n",
            "[INFO] AgentBehavior. Step: 1500000. Time Elapsed: 9592.988 s. Mean Reward: 0.062. Std of Reward: 2.048. Training.\n",
            "[INFO] Exported results/ColabTest/AgentBehavior/AgentBehavior-1499982.onnx\n",
            "[INFO] AgentBehavior. Step: 1510000. Time Elapsed: 9654.676 s. Mean Reward: 0.543. Std of Reward: 2.363. Training.\n",
            "[INFO] AgentBehavior. Step: 1520000. Time Elapsed: 9719.672 s. Mean Reward: 0.480. Std of Reward: 2.272. Training.\n",
            "[INFO] AgentBehavior. Step: 1530000. Time Elapsed: 9785.045 s. Mean Reward: 0.414. Std of Reward: 2.122. Training.\n",
            "[INFO] AgentBehavior. Step: 1540000. Time Elapsed: 9850.827 s. Mean Reward: 0.312. Std of Reward: 1.997. Training.\n",
            "[INFO] AgentBehavior. Step: 1550000. Time Elapsed: 9918.048 s. Mean Reward: 0.135. Std of Reward: 2.209. Training.\n",
            "[INFO] AgentBehavior. Step: 1560000. Time Elapsed: 9979.637 s. Mean Reward: 0.236. Std of Reward: 2.021. Training.\n",
            "[INFO] AgentBehavior. Step: 1570000. Time Elapsed: 10045.196 s. Mean Reward: 0.256. Std of Reward: 1.896. Training.\n",
            "[INFO] AgentBehavior. Step: 1580000. Time Elapsed: 10109.334 s. Mean Reward: 0.351. Std of Reward: 2.131. Training.\n",
            "[INFO] AgentBehavior. Step: 1590000. Time Elapsed: 10174.807 s. Mean Reward: -0.019. Std of Reward: 1.871. Training.\n",
            "[INFO] AgentBehavior. Step: 1600000. Time Elapsed: 10241.645 s. Mean Reward: -0.006. Std of Reward: 1.886. Training.\n",
            "[INFO] AgentBehavior. Step: 1610000. Time Elapsed: 10305.524 s. Mean Reward: 0.344. Std of Reward: 1.951. Training.\n",
            "[INFO] AgentBehavior. Step: 1620000. Time Elapsed: 10370.780 s. Mean Reward: 0.370. Std of Reward: 2.135. Training.\n",
            "[INFO] AgentBehavior. Step: 1630000. Time Elapsed: 10434.763 s. Mean Reward: 0.319. Std of Reward: 2.446. Training.\n",
            "[INFO] AgentBehavior. Step: 1640000. Time Elapsed: 10498.730 s. Mean Reward: 0.472. Std of Reward: 2.073. Training.\n",
            "[INFO] AgentBehavior. Step: 1650000. Time Elapsed: 10564.896 s. Mean Reward: 0.187. Std of Reward: 2.035. Training.\n",
            "[INFO] AgentBehavior. Step: 1660000. Time Elapsed: 10628.587 s. Mean Reward: 0.496. Std of Reward: 2.039. Training.\n",
            "[INFO] AgentBehavior. Step: 1670000. Time Elapsed: 10694.416 s. Mean Reward: 0.484. Std of Reward: 2.197. Training.\n",
            "[INFO] AgentBehavior. Step: 1680000. Time Elapsed: 10761.075 s. Mean Reward: 0.476. Std of Reward: 2.144. Training.\n",
            "[INFO] AgentBehavior. Step: 1690000. Time Elapsed: 10823.761 s. Mean Reward: 0.239. Std of Reward: 2.023. Training.\n",
            "[INFO] AgentBehavior. Step: 1700000. Time Elapsed: 10888.520 s. Mean Reward: 0.571. Std of Reward: 2.126. Training.\n",
            "[INFO] AgentBehavior. Step: 1710000. Time Elapsed: 10952.244 s. Mean Reward: 0.021. Std of Reward: 1.803. Training.\n",
            "[INFO] AgentBehavior. Step: 1720000. Time Elapsed: 11017.048 s. Mean Reward: 0.145. Std of Reward: 1.799. Training.\n",
            "[INFO] AgentBehavior. Step: 1730000. Time Elapsed: 11082.390 s. Mean Reward: 0.365. Std of Reward: 2.039. Training.\n",
            "[INFO] AgentBehavior. Step: 1740000. Time Elapsed: 11146.887 s. Mean Reward: 0.542. Std of Reward: 2.165. Training.\n",
            "[INFO] AgentBehavior. Step: 1750000. Time Elapsed: 11213.052 s. Mean Reward: 0.480. Std of Reward: 1.995. Training.\n",
            "[INFO] AgentBehavior. Step: 1760000. Time Elapsed: 11278.646 s. Mean Reward: 0.203. Std of Reward: 1.848. Training.\n",
            "[INFO] AgentBehavior. Step: 1770000. Time Elapsed: 11343.864 s. Mean Reward: 0.465. Std of Reward: 2.130. Training.\n",
            "[INFO] AgentBehavior. Step: 1780000. Time Elapsed: 11407.884 s. Mean Reward: 0.418. Std of Reward: 2.283. Training.\n",
            "[INFO] AgentBehavior. Step: 1790000. Time Elapsed: 11471.403 s. Mean Reward: 0.319. Std of Reward: 2.133. Training.\n",
            "[INFO] AgentBehavior. Step: 1800000. Time Elapsed: 11536.800 s. Mean Reward: 0.306. Std of Reward: 2.028. Training.\n",
            "[INFO] AgentBehavior. Step: 1810000. Time Elapsed: 11602.224 s. Mean Reward: 0.832. Std of Reward: 2.129. Training.\n",
            "[INFO] AgentBehavior. Step: 1820000. Time Elapsed: 11666.329 s. Mean Reward: 0.243. Std of Reward: 2.018. Training.\n",
            "[INFO] AgentBehavior. Step: 1830000. Time Elapsed: 11731.657 s. Mean Reward: -0.020. Std of Reward: 2.082. Training.\n",
            "[INFO] AgentBehavior. Step: 1840000. Time Elapsed: 11794.441 s. Mean Reward: 0.618. Std of Reward: 2.110. Training.\n",
            "[INFO] AgentBehavior. Step: 1850000. Time Elapsed: 11859.421 s. Mean Reward: 0.257. Std of Reward: 2.159. Training.\n",
            "[INFO] AgentBehavior. Step: 1860000. Time Elapsed: 11925.849 s. Mean Reward: 0.134. Std of Reward: 1.983. Training.\n",
            "[INFO] AgentBehavior. Step: 1870000. Time Elapsed: 11990.590 s. Mean Reward: 0.696. Std of Reward: 2.233. Training.\n",
            "[INFO] AgentBehavior. Step: 1880000. Time Elapsed: 12054.916 s. Mean Reward: -0.286. Std of Reward: 2.357. Training.\n",
            "[INFO] AgentBehavior. Step: 1890000. Time Elapsed: 12122.431 s. Mean Reward: 0.297. Std of Reward: 2.256. Training.\n",
            "[INFO] AgentBehavior. Step: 1900000. Time Elapsed: 12187.425 s. Mean Reward: 0.516. Std of Reward: 2.214. Training.\n",
            "[INFO] AgentBehavior. Step: 1910000. Time Elapsed: 12252.476 s. Mean Reward: 0.165. Std of Reward: 1.997. Training.\n",
            "[INFO] AgentBehavior. Step: 1920000. Time Elapsed: 12317.274 s. Mean Reward: 0.623. Std of Reward: 2.175. Training.\n",
            "[INFO] AgentBehavior. Step: 1930000. Time Elapsed: 12382.785 s. Mean Reward: 0.396. Std of Reward: 1.925. Training.\n",
            "[INFO] AgentBehavior. Step: 1940000. Time Elapsed: 12449.659 s. Mean Reward: 0.169. Std of Reward: 2.177. Training.\n",
            "[INFO] AgentBehavior. Step: 1950000. Time Elapsed: 12513.428 s. Mean Reward: 0.543. Std of Reward: 2.260. Training.\n",
            "[INFO] AgentBehavior. Step: 1960000. Time Elapsed: 12578.823 s. Mean Reward: 0.575. Std of Reward: 2.115. Training.\n",
            "[INFO] AgentBehavior. Step: 1970000. Time Elapsed: 12643.754 s. Mean Reward: 0.259. Std of Reward: 2.124. Training.\n",
            "[INFO] AgentBehavior. Step: 1980000. Time Elapsed: 12708.669 s. Mean Reward: 0.685. Std of Reward: 2.241. Training.\n",
            "[INFO] AgentBehavior. Step: 1990000. Time Elapsed: 12774.608 s. Mean Reward: 1.141. Std of Reward: 2.229. Training.\n",
            "[INFO] AgentBehavior. Step: 2000000. Time Elapsed: 12837.095 s. Mean Reward: 0.556. Std of Reward: 2.045. Training.\n",
            "[INFO] Exported results/ColabTest/AgentBehavior/AgentBehavior-1999978.onnx\n",
            "[INFO] AgentBehavior. Step: 2010000. Time Elapsed: 12902.808 s. Mean Reward: 0.859. Std of Reward: 2.123. Training.\n",
            "[INFO] AgentBehavior. Step: 2020000. Time Elapsed: 12967.352 s. Mean Reward: 0.478. Std of Reward: 2.124. Training.\n",
            "[INFO] AgentBehavior. Step: 2030000. Time Elapsed: 13030.726 s. Mean Reward: 0.781. Std of Reward: 2.071. Training.\n",
            "[INFO] AgentBehavior. Step: 2040000. Time Elapsed: 13095.718 s. Mean Reward: 0.191. Std of Reward: 2.099. Training.\n",
            "[INFO] AgentBehavior. Step: 2050000. Time Elapsed: 13157.524 s. Mean Reward: 0.671. Std of Reward: 2.189. Training.\n",
            "[INFO] AgentBehavior. Step: 2060000. Time Elapsed: 13223.370 s. Mean Reward: 0.836. Std of Reward: 2.219. Training.\n",
            "[INFO] AgentBehavior. Step: 2070000. Time Elapsed: 13288.126 s. Mean Reward: 0.799. Std of Reward: 2.217. Training.\n",
            "[INFO] AgentBehavior. Step: 2080000. Time Elapsed: 13350.784 s. Mean Reward: 0.636. Std of Reward: 2.057. Training.\n",
            "[INFO] AgentBehavior. Step: 2090000. Time Elapsed: 13415.982 s. Mean Reward: 0.856. Std of Reward: 2.182. Training.\n",
            "[INFO] AgentBehavior. Step: 2100000. Time Elapsed: 13479.885 s. Mean Reward: 0.675. Std of Reward: 2.111. Training.\n",
            "[INFO] AgentBehavior. Step: 2110000. Time Elapsed: 13543.793 s. Mean Reward: 1.094. Std of Reward: 2.187. Training.\n",
            "[INFO] AgentBehavior. Step: 2120000. Time Elapsed: 13609.497 s. Mean Reward: 0.911. Std of Reward: 2.274. Training.\n",
            "[INFO] AgentBehavior. Step: 2130000. Time Elapsed: 13674.769 s. Mean Reward: 0.566. Std of Reward: 2.084. Training.\n",
            "[INFO] AgentBehavior. Step: 2140000. Time Elapsed: 13739.924 s. Mean Reward: 0.530. Std of Reward: 1.996. Training.\n",
            "[INFO] AgentBehavior. Step: 2150000. Time Elapsed: 13806.234 s. Mean Reward: 0.675. Std of Reward: 2.148. Training.\n",
            "[INFO] AgentBehavior. Step: 2160000. Time Elapsed: 13869.385 s. Mean Reward: 0.555. Std of Reward: 2.096. Training.\n",
            "[INFO] AgentBehavior. Step: 2170000. Time Elapsed: 13934.286 s. Mean Reward: 0.044. Std of Reward: 2.085. Training.\n",
            "[INFO] AgentBehavior. Step: 2180000. Time Elapsed: 13998.719 s. Mean Reward: 0.314. Std of Reward: 2.111. Training.\n",
            "[INFO] AgentBehavior. Step: 2190000. Time Elapsed: 14065.523 s. Mean Reward: 0.206. Std of Reward: 1.904. Training.\n",
            "[INFO] AgentBehavior. Step: 2200000. Time Elapsed: 14132.605 s. Mean Reward: 0.679. Std of Reward: 2.146. Training.\n",
            "[INFO] AgentBehavior. Step: 2210000. Time Elapsed: 14197.472 s. Mean Reward: 0.633. Std of Reward: 2.208. Training.\n",
            "[INFO] AgentBehavior. Step: 2220000. Time Elapsed: 14263.867 s. Mean Reward: 0.529. Std of Reward: 2.343. Training.\n",
            "[INFO] AgentBehavior. Step: 2230000. Time Elapsed: 14329.646 s. Mean Reward: 0.476. Std of Reward: 2.154. Training.\n",
            "[INFO] AgentBehavior. Step: 2240000. Time Elapsed: 14393.984 s. Mean Reward: 1.023. Std of Reward: 2.051. Training.\n",
            "[INFO] AgentBehavior. Step: 2250000. Time Elapsed: 14459.485 s. Mean Reward: 0.355. Std of Reward: 1.933. Training.\n",
            "[INFO] AgentBehavior. Step: 2260000. Time Elapsed: 14521.762 s. Mean Reward: 0.463. Std of Reward: 2.246. Training.\n",
            "[INFO] AgentBehavior. Step: 2270000. Time Elapsed: 14587.061 s. Mean Reward: 0.439. Std of Reward: 2.133. Training.\n",
            "[INFO] AgentBehavior. Step: 2280000. Time Elapsed: 14653.094 s. Mean Reward: 0.406. Std of Reward: 2.617. Training.\n",
            "[INFO] AgentBehavior. Step: 2290000. Time Elapsed: 14718.435 s. Mean Reward: 0.468. Std of Reward: 2.157. Training.\n",
            "[INFO] AgentBehavior. Step: 2300000. Time Elapsed: 14786.140 s. Mean Reward: 0.360. Std of Reward: 2.009. Training.\n",
            "[INFO] AgentBehavior. Step: 2310000. Time Elapsed: 14851.508 s. Mean Reward: 0.578. Std of Reward: 1.978. Training.\n",
            "[INFO] AgentBehavior. Step: 2320000. Time Elapsed: 14916.566 s. Mean Reward: 0.426. Std of Reward: 2.111. Training.\n",
            "[INFO] AgentBehavior. Step: 2330000. Time Elapsed: 14982.704 s. Mean Reward: 0.428. Std of Reward: 2.023. Training.\n",
            "[INFO] AgentBehavior. Step: 2340000. Time Elapsed: 15046.579 s. Mean Reward: 0.902. Std of Reward: 2.239. Training.\n",
            "[INFO] AgentBehavior. Step: 2350000. Time Elapsed: 15111.421 s. Mean Reward: 0.680. Std of Reward: 2.265. Training.\n",
            "[INFO] AgentBehavior. Step: 2360000. Time Elapsed: 15174.814 s. Mean Reward: 0.087. Std of Reward: 1.660. Training.\n",
            "[INFO] AgentBehavior. Step: 2370000. Time Elapsed: 15237.585 s. Mean Reward: -0.067. Std of Reward: 1.802. Training.\n",
            "[INFO] AgentBehavior. Step: 2380000. Time Elapsed: 15303.052 s. Mean Reward: 0.334. Std of Reward: 2.138. Training.\n",
            "[INFO] AgentBehavior. Step: 2390000. Time Elapsed: 15366.337 s. Mean Reward: 0.210. Std of Reward: 2.070. Training.\n",
            "[INFO] AgentBehavior. Step: 2400000. Time Elapsed: 15433.096 s. Mean Reward: 0.333. Std of Reward: 2.171. Training.\n",
            "[INFO] AgentBehavior. Step: 2410000. Time Elapsed: 15497.505 s. Mean Reward: 0.545. Std of Reward: 2.365. Training.\n",
            "[INFO] AgentBehavior. Step: 2420000. Time Elapsed: 15559.662 s. Mean Reward: 0.561. Std of Reward: 2.295. Training.\n",
            "[INFO] AgentBehavior. Step: 2430000. Time Elapsed: 15624.477 s. Mean Reward: 0.844. Std of Reward: 2.250. Training.\n",
            "[INFO] AgentBehavior. Step: 2440000. Time Elapsed: 15688.577 s. Mean Reward: 0.600. Std of Reward: 2.255. Training.\n",
            "[INFO] AgentBehavior. Step: 2450000. Time Elapsed: 15751.347 s. Mean Reward: 0.751. Std of Reward: 2.255. Training.\n",
            "[INFO] AgentBehavior. Step: 2460000. Time Elapsed: 15816.537 s. Mean Reward: 0.612. Std of Reward: 2.116. Training.\n",
            "[INFO] AgentBehavior. Step: 2470000. Time Elapsed: 15878.876 s. Mean Reward: 0.261. Std of Reward: 1.762. Training.\n",
            "[INFO] AgentBehavior. Step: 2480000. Time Elapsed: 15943.138 s. Mean Reward: 0.373. Std of Reward: 2.048. Training.\n",
            "[INFO] AgentBehavior. Step: 2490000. Time Elapsed: 16005.984 s. Mean Reward: 0.963. Std of Reward: 2.248. Training.\n",
            "[INFO] AgentBehavior. Step: 2500000. Time Elapsed: 16070.834 s. Mean Reward: 0.452. Std of Reward: 1.922. Training.\n",
            "[INFO] Exported results/ColabTest/AgentBehavior/AgentBehavior-2499997.onnx\n",
            "[INFO] AgentBehavior. Step: 2510000. Time Elapsed: 16138.216 s. Mean Reward: 0.661. Std of Reward: 2.227. Training.\n",
            "[INFO] AgentBehavior. Step: 2520000. Time Elapsed: 16203.362 s. Mean Reward: 0.479. Std of Reward: 2.002. Training.\n",
            "[INFO] AgentBehavior. Step: 2530000. Time Elapsed: 16270.057 s. Mean Reward: 0.152. Std of Reward: 1.921. Training.\n",
            "[INFO] AgentBehavior. Step: 2540000. Time Elapsed: 16336.316 s. Mean Reward: 0.483. Std of Reward: 2.197. Training.\n",
            "[INFO] AgentBehavior. Step: 2550000. Time Elapsed: 16403.987 s. Mean Reward: 0.105. Std of Reward: 2.105. Training.\n",
            "[INFO] AgentBehavior. Step: 2560000. Time Elapsed: 16471.025 s. Mean Reward: 0.201. Std of Reward: 2.039. Training.\n",
            "[INFO] AgentBehavior. Step: 2570000. Time Elapsed: 16536.016 s. Mean Reward: 0.808. Std of Reward: 2.092. Training.\n",
            "[INFO] AgentBehavior. Step: 2580000. Time Elapsed: 16601.519 s. Mean Reward: 0.693. Std of Reward: 2.100. Training.\n",
            "[INFO] AgentBehavior. Step: 2590000. Time Elapsed: 16668.282 s. Mean Reward: 0.484. Std of Reward: 2.066. Training.\n",
            "[INFO] AgentBehavior. Step: 2600000. Time Elapsed: 16734.041 s. Mean Reward: 0.577. Std of Reward: 2.190. Training.\n",
            "[INFO] AgentBehavior. Step: 2610000. Time Elapsed: 16800.918 s. Mean Reward: 0.777. Std of Reward: 2.331. Training.\n",
            "[INFO] AgentBehavior. Step: 2620000. Time Elapsed: 16863.995 s. Mean Reward: 0.818. Std of Reward: 2.159. Training.\n",
            "[INFO] AgentBehavior. Step: 2630000. Time Elapsed: 16931.333 s. Mean Reward: 0.616. Std of Reward: 2.111. Training.\n",
            "[INFO] AgentBehavior. Step: 2640000. Time Elapsed: 16996.093 s. Mean Reward: 0.894. Std of Reward: 2.148. Training.\n",
            "[INFO] AgentBehavior. Step: 2650000. Time Elapsed: 17062.172 s. Mean Reward: 0.706. Std of Reward: 2.137. Training.\n",
            "[INFO] AgentBehavior. Step: 2660000. Time Elapsed: 17127.942 s. Mean Reward: 0.511. Std of Reward: 2.141. Training.\n",
            "[INFO] AgentBehavior. Step: 2670000. Time Elapsed: 17191.526 s. Mean Reward: 0.304. Std of Reward: 2.077. Training.\n",
            "[INFO] AgentBehavior. Step: 2680000. Time Elapsed: 17259.149 s. Mean Reward: 0.783. Std of Reward: 2.112. Training.\n",
            "[INFO] AgentBehavior. Step: 2690000. Time Elapsed: 17323.593 s. Mean Reward: 0.309. Std of Reward: 1.940. Training.\n",
            "[INFO] AgentBehavior. Step: 2700000. Time Elapsed: 17387.312 s. Mean Reward: 0.478. Std of Reward: 2.163. Training.\n",
            "[INFO] AgentBehavior. Step: 2710000. Time Elapsed: 17453.982 s. Mean Reward: 1.085. Std of Reward: 2.202. Training.\n",
            "[INFO] AgentBehavior. Step: 2720000. Time Elapsed: 17518.063 s. Mean Reward: 0.624. Std of Reward: 2.284. Training.\n",
            "[INFO] AgentBehavior. Step: 2730000. Time Elapsed: 17583.410 s. Mean Reward: -0.160. Std of Reward: 1.592. Training.\n",
            "[INFO] AgentBehavior. Step: 2740000. Time Elapsed: 17647.791 s. Mean Reward: 0.333. Std of Reward: 2.603. Training.\n",
            "[INFO] AgentBehavior. Step: 2750000. Time Elapsed: 17713.189 s. Mean Reward: 0.347. Std of Reward: 2.003. Training.\n",
            "[INFO] AgentBehavior. Step: 2760000. Time Elapsed: 17778.961 s. Mean Reward: 0.998. Std of Reward: 2.200. Training.\n",
            "[INFO] AgentBehavior. Step: 2770000. Time Elapsed: 17841.843 s. Mean Reward: 0.461. Std of Reward: 2.108. Training.\n",
            "[INFO] AgentBehavior. Step: 2780000. Time Elapsed: 17905.157 s. Mean Reward: 0.386. Std of Reward: 2.205. Training.\n",
            "[INFO] AgentBehavior. Step: 2790000. Time Elapsed: 17969.394 s. Mean Reward: 0.801. Std of Reward: 1.993. Training.\n",
            "[INFO] AgentBehavior. Step: 2800000. Time Elapsed: 18032.966 s. Mean Reward: 0.309. Std of Reward: 1.996. Training.\n",
            "[INFO] AgentBehavior. Step: 2810000. Time Elapsed: 18096.122 s. Mean Reward: 0.572. Std of Reward: 2.163. Training.\n",
            "[INFO] AgentBehavior. Step: 2820000. Time Elapsed: 18159.219 s. Mean Reward: 0.692. Std of Reward: 2.042. Training.\n",
            "[INFO] AgentBehavior. Step: 2830000. Time Elapsed: 18222.828 s. Mean Reward: 0.212. Std of Reward: 1.787. Training.\n",
            "[INFO] AgentBehavior. Step: 2840000. Time Elapsed: 18285.247 s. Mean Reward: 0.443. Std of Reward: 2.038. Training.\n",
            "[INFO] AgentBehavior. Step: 2850000. Time Elapsed: 18348.423 s. Mean Reward: 1.151. Std of Reward: 2.327. Training.\n",
            "[INFO] AgentBehavior. Step: 2860000. Time Elapsed: 18412.188 s. Mean Reward: 0.679. Std of Reward: 1.997. Training.\n",
            "[INFO] AgentBehavior. Step: 2870000. Time Elapsed: 18474.799 s. Mean Reward: 0.538. Std of Reward: 1.975. Training.\n",
            "[INFO] AgentBehavior. Step: 2880000. Time Elapsed: 18537.788 s. Mean Reward: 0.790. Std of Reward: 2.276. Training.\n",
            "[INFO] AgentBehavior. Step: 2890000. Time Elapsed: 18599.776 s. Mean Reward: 0.740. Std of Reward: 2.192. Training.\n",
            "[INFO] AgentBehavior. Step: 2900000. Time Elapsed: 18663.189 s. Mean Reward: 0.637. Std of Reward: 2.324. Training.\n",
            "[INFO] AgentBehavior. Step: 2910000. Time Elapsed: 18725.285 s. Mean Reward: 0.700. Std of Reward: 2.258. Training.\n",
            "[INFO] AgentBehavior. Step: 2920000. Time Elapsed: 18788.784 s. Mean Reward: 0.637. Std of Reward: 2.251. Training.\n",
            "[INFO] AgentBehavior. Step: 2930000. Time Elapsed: 18852.331 s. Mean Reward: 0.799. Std of Reward: 2.135. Training.\n",
            "[INFO] AgentBehavior. Step: 2940000. Time Elapsed: 18918.477 s. Mean Reward: 0.333. Std of Reward: 1.957. Training.\n",
            "[INFO] AgentBehavior. Step: 2950000. Time Elapsed: 18985.104 s. Mean Reward: 0.891. Std of Reward: 2.281. Training.\n",
            "[INFO] AgentBehavior. Step: 2960000. Time Elapsed: 19048.737 s. Mean Reward: 0.514. Std of Reward: 2.174. Training.\n",
            "[INFO] AgentBehavior. Step: 2970000. Time Elapsed: 19113.115 s. Mean Reward: 0.448. Std of Reward: 2.154. Training.\n",
            "[INFO] AgentBehavior. Step: 2980000. Time Elapsed: 19178.427 s. Mean Reward: 0.921. Std of Reward: 1.882. Training.\n",
            "[INFO] AgentBehavior. Step: 2990000. Time Elapsed: 19242.905 s. Mean Reward: 0.570. Std of Reward: 2.201. Training.\n",
            "[INFO] AgentBehavior. Step: 3000000. Time Elapsed: 19307.928 s. Mean Reward: 0.761. Std of Reward: 2.251. Training.\n",
            "[INFO] Exported results/ColabTest/AgentBehavior/AgentBehavior-2999942.onnx\n",
            "[INFO] AgentBehavior. Step: 3010000. Time Elapsed: 19369.319 s. Mean Reward: 0.984. Std of Reward: 2.291. Training.\n",
            "[INFO] AgentBehavior. Step: 3020000. Time Elapsed: 19433.641 s. Mean Reward: 0.643. Std of Reward: 2.180. Training.\n",
            "[INFO] AgentBehavior. Step: 3030000. Time Elapsed: 19496.928 s. Mean Reward: 1.566. Std of Reward: 2.409. Training.\n",
            "[INFO] AgentBehavior. Step: 3040000. Time Elapsed: 19561.926 s. Mean Reward: 1.036. Std of Reward: 2.313. Training.\n",
            "[INFO] AgentBehavior. Step: 3050000. Time Elapsed: 19626.648 s. Mean Reward: 0.429. Std of Reward: 2.088. Training.\n",
            "[INFO] AgentBehavior. Step: 3060000. Time Elapsed: 19689.837 s. Mean Reward: 0.720. Std of Reward: 2.306. Training.\n",
            "[INFO] AgentBehavior. Step: 3070000. Time Elapsed: 19754.618 s. Mean Reward: 1.027. Std of Reward: 2.255. Training.\n",
            "[INFO] AgentBehavior. Step: 3080000. Time Elapsed: 19817.199 s. Mean Reward: 0.451. Std of Reward: 1.979. Training.\n",
            "[INFO] AgentBehavior. Step: 3090000. Time Elapsed: 19882.356 s. Mean Reward: 0.246. Std of Reward: 2.356. Training.\n",
            "[INFO] AgentBehavior. Step: 3100000. Time Elapsed: 19945.695 s. Mean Reward: 0.802. Std of Reward: 2.113. Training.\n",
            "[INFO] AgentBehavior. Step: 3110000. Time Elapsed: 20010.527 s. Mean Reward: 0.812. Std of Reward: 2.199. Training.\n",
            "[INFO] AgentBehavior. Step: 3120000. Time Elapsed: 20072.110 s. Mean Reward: 0.746. Std of Reward: 2.255. Training.\n",
            "[INFO] AgentBehavior. Step: 3130000. Time Elapsed: 20136.926 s. Mean Reward: 0.822. Std of Reward: 2.254. Training.\n",
            "[INFO] AgentBehavior. Step: 3140000. Time Elapsed: 20202.653 s. Mean Reward: 0.698. Std of Reward: 2.108. Training.\n",
            "[INFO] AgentBehavior. Step: 3150000. Time Elapsed: 20267.398 s. Mean Reward: 0.876. Std of Reward: 2.351. Training.\n",
            "[INFO] AgentBehavior. Step: 3160000. Time Elapsed: 20332.525 s. Mean Reward: 0.528. Std of Reward: 2.116. Training.\n",
            "[INFO] AgentBehavior. Step: 3170000. Time Elapsed: 20394.937 s. Mean Reward: 0.239. Std of Reward: 1.796. Training.\n",
            "[INFO] AgentBehavior. Step: 3180000. Time Elapsed: 20460.453 s. Mean Reward: 0.817. Std of Reward: 2.290. Training.\n",
            "[INFO] AgentBehavior. Step: 3190000. Time Elapsed: 20524.774 s. Mean Reward: 0.539. Std of Reward: 2.093. Training.\n",
            "[INFO] AgentBehavior. Step: 3200000. Time Elapsed: 20592.636 s. Mean Reward: 0.673. Std of Reward: 2.210. Training.\n",
            "[INFO] AgentBehavior. Step: 3210000. Time Elapsed: 20665.499 s. Mean Reward: 0.826. Std of Reward: 2.370. Training.\n",
            "[INFO] AgentBehavior. Step: 3220000. Time Elapsed: 20730.220 s. Mean Reward: 0.655. Std of Reward: 2.250. Training.\n",
            "[INFO] AgentBehavior. Step: 3230000. Time Elapsed: 20796.103 s. Mean Reward: 1.073. Std of Reward: 2.280. Training.\n",
            "[INFO] AgentBehavior. Step: 3240000. Time Elapsed: 20859.595 s. Mean Reward: 0.723. Std of Reward: 2.180. Training.\n",
            "[INFO] AgentBehavior. Step: 3250000. Time Elapsed: 20924.458 s. Mean Reward: 0.718. Std of Reward: 2.123. Training.\n",
            "[INFO] AgentBehavior. Step: 3260000. Time Elapsed: 20989.579 s. Mean Reward: 0.793. Std of Reward: 2.215. Training.\n",
            "[INFO] AgentBehavior. Step: 3270000. Time Elapsed: 21053.323 s. Mean Reward: 0.647. Std of Reward: 2.133. Training.\n",
            "[INFO] AgentBehavior. Step: 3280000. Time Elapsed: 21120.441 s. Mean Reward: 1.001. Std of Reward: 2.194. Training.\n",
            "[INFO] AgentBehavior. Step: 3290000. Time Elapsed: 21187.146 s. Mean Reward: 0.884. Std of Reward: 2.230. Training.\n",
            "[INFO] AgentBehavior. Step: 3300000. Time Elapsed: 21254.788 s. Mean Reward: 1.079. Std of Reward: 2.149. Training.\n",
            "[INFO] AgentBehavior. Step: 3310000. Time Elapsed: 21321.384 s. Mean Reward: 0.472. Std of Reward: 2.131. Training.\n",
            "[INFO] AgentBehavior. Step: 3320000. Time Elapsed: 21390.118 s. Mean Reward: 0.905. Std of Reward: 2.154. Training.\n",
            "[INFO] AgentBehavior. Step: 3330000. Time Elapsed: 21457.599 s. Mean Reward: 0.283. Std of Reward: 2.042. Training.\n",
            "[INFO] AgentBehavior. Step: 3340000. Time Elapsed: 21522.547 s. Mean Reward: 0.766. Std of Reward: 2.309. Training.\n",
            "[INFO] AgentBehavior. Step: 3350000. Time Elapsed: 21589.496 s. Mean Reward: 0.934. Std of Reward: 2.229. Training.\n",
            "[INFO] AgentBehavior. Step: 3360000. Time Elapsed: 21654.080 s. Mean Reward: 0.425. Std of Reward: 2.154. Training.\n",
            "[INFO] AgentBehavior. Step: 3370000. Time Elapsed: 21720.110 s. Mean Reward: 0.268. Std of Reward: 2.011. Training.\n",
            "[INFO] AgentBehavior. Step: 3380000. Time Elapsed: 21785.439 s. Mean Reward: 0.920. Std of Reward: 2.259. Training.\n",
            "[INFO] AgentBehavior. Step: 3390000. Time Elapsed: 21850.335 s. Mean Reward: 0.914. Std of Reward: 2.276. Training.\n",
            "[INFO] AgentBehavior. Step: 3400000. Time Elapsed: 21914.107 s. Mean Reward: 1.296. Std of Reward: 2.289. Training.\n",
            "[INFO] AgentBehavior. Step: 3410000. Time Elapsed: 21979.422 s. Mean Reward: 0.701. Std of Reward: 2.176. Training.\n",
            "[INFO] AgentBehavior. Step: 3420000. Time Elapsed: 22046.588 s. Mean Reward: 0.674. Std of Reward: 1.989. Training.\n",
            "[INFO] AgentBehavior. Step: 3430000. Time Elapsed: 22111.721 s. Mean Reward: 0.527. Std of Reward: 2.300. Training.\n",
            "[INFO] AgentBehavior. Step: 3440000. Time Elapsed: 22178.106 s. Mean Reward: 0.613. Std of Reward: 2.243. Training.\n",
            "[INFO] AgentBehavior. Step: 3450000. Time Elapsed: 22242.532 s. Mean Reward: 0.255. Std of Reward: 2.321. Training.\n",
            "[INFO] AgentBehavior. Step: 3460000. Time Elapsed: 22309.210 s. Mean Reward: 0.148. Std of Reward: 1.959. Training.\n",
            "[INFO] AgentBehavior. Step: 3470000. Time Elapsed: 22374.444 s. Mean Reward: 1.244. Std of Reward: 2.248. Training.\n",
            "[INFO] AgentBehavior. Step: 3480000. Time Elapsed: 22440.377 s. Mean Reward: 0.595. Std of Reward: 2.156. Training.\n",
            "[INFO] AgentBehavior. Step: 3490000. Time Elapsed: 22503.404 s. Mean Reward: 0.611. Std of Reward: 2.224. Training.\n",
            "[INFO] AgentBehavior. Step: 3500000. Time Elapsed: 22570.301 s. Mean Reward: 0.408. Std of Reward: 2.119. Training.\n",
            "[INFO] Exported results/ColabTest/AgentBehavior/AgentBehavior-3499998.onnx\n",
            "[INFO] AgentBehavior. Step: 3510000. Time Elapsed: 22635.082 s. Mean Reward: 0.481. Std of Reward: 2.166. Training.\n",
            "[INFO] AgentBehavior. Step: 3520000. Time Elapsed: 22700.379 s. Mean Reward: 0.939. Std of Reward: 2.274. Training.\n",
            "[INFO] AgentBehavior. Step: 3530000. Time Elapsed: 22766.778 s. Mean Reward: 0.382. Std of Reward: 2.162. Training.\n",
            "[INFO] AgentBehavior. Step: 3540000. Time Elapsed: 22831.845 s. Mean Reward: 0.438. Std of Reward: 2.134. Training.\n",
            "[INFO] AgentBehavior. Step: 3550000. Time Elapsed: 22897.870 s. Mean Reward: 1.042. Std of Reward: 2.299. Training.\n",
            "[INFO] AgentBehavior. Step: 3560000. Time Elapsed: 22960.703 s. Mean Reward: 0.562. Std of Reward: 2.108. Training.\n",
            "[INFO] AgentBehavior. Step: 3570000. Time Elapsed: 23027.201 s. Mean Reward: 1.381. Std of Reward: 2.377. Training.\n",
            "[INFO] AgentBehavior. Step: 3580000. Time Elapsed: 23092.001 s. Mean Reward: 1.010. Std of Reward: 2.208. Training.\n",
            "[INFO] AgentBehavior. Step: 3590000. Time Elapsed: 23158.238 s. Mean Reward: 0.886. Std of Reward: 2.294. Training.\n",
            "[INFO] AgentBehavior. Step: 3600000. Time Elapsed: 23222.405 s. Mean Reward: 0.376. Std of Reward: 2.268. Training.\n",
            "[INFO] AgentBehavior. Step: 3610000. Time Elapsed: 23288.357 s. Mean Reward: 0.834. Std of Reward: 2.364. Training.\n",
            "[INFO] AgentBehavior. Step: 3620000. Time Elapsed: 23352.515 s. Mean Reward: 0.857. Std of Reward: 2.173. Training.\n",
            "[INFO] AgentBehavior. Step: 3630000. Time Elapsed: 23416.598 s. Mean Reward: 0.784. Std of Reward: 2.298. Training.\n",
            "[INFO] AgentBehavior. Step: 3640000. Time Elapsed: 23480.451 s. Mean Reward: 0.541. Std of Reward: 2.098. Training.\n",
            "[INFO] AgentBehavior. Step: 3650000. Time Elapsed: 23545.966 s. Mean Reward: 0.505. Std of Reward: 2.100. Training.\n",
            "[INFO] AgentBehavior. Step: 3660000. Time Elapsed: 23612.567 s. Mean Reward: 0.323. Std of Reward: 1.931. Training.\n",
            "[INFO] AgentBehavior. Step: 3670000. Time Elapsed: 23676.685 s. Mean Reward: 0.958. Std of Reward: 2.075. Training.\n",
            "[INFO] AgentBehavior. Step: 3680000. Time Elapsed: 23742.840 s. Mean Reward: 0.984. Std of Reward: 2.109. Training.\n",
            "[INFO] AgentBehavior. Step: 3690000. Time Elapsed: 23806.806 s. Mean Reward: 0.949. Std of Reward: 2.290. Training.\n",
            "[INFO] AgentBehavior. Step: 3700000. Time Elapsed: 23872.757 s. Mean Reward: 0.512. Std of Reward: 2.197. Training.\n",
            "[INFO] AgentBehavior. Step: 3710000. Time Elapsed: 23936.107 s. Mean Reward: 1.387. Std of Reward: 2.233. Training.\n",
            "[INFO] AgentBehavior. Step: 3720000. Time Elapsed: 24003.042 s. Mean Reward: 0.866. Std of Reward: 2.419. Training.\n",
            "[INFO] AgentBehavior. Step: 3730000. Time Elapsed: 24067.691 s. Mean Reward: 0.663. Std of Reward: 1.972. Training.\n",
            "[INFO] AgentBehavior. Step: 3740000. Time Elapsed: 24133.319 s. Mean Reward: 1.073. Std of Reward: 2.193. Training.\n",
            "[INFO] AgentBehavior. Step: 3750000. Time Elapsed: 24197.478 s. Mean Reward: 0.775. Std of Reward: 2.105. Training.\n",
            "[INFO] AgentBehavior. Step: 3760000. Time Elapsed: 24262.793 s. Mean Reward: 0.803. Std of Reward: 2.113. Training.\n",
            "[INFO] AgentBehavior. Step: 3770000. Time Elapsed: 24323.743 s. Mean Reward: 0.486. Std of Reward: 2.067. Training.\n",
            "[INFO] AgentBehavior. Step: 3780000. Time Elapsed: 24389.771 s. Mean Reward: 0.185. Std of Reward: 1.790. Training.\n",
            "[INFO] AgentBehavior. Step: 3790000. Time Elapsed: 24453.951 s. Mean Reward: 0.895. Std of Reward: 2.092. Training.\n",
            "[INFO] AgentBehavior. Step: 3800000. Time Elapsed: 24519.240 s. Mean Reward: 0.440. Std of Reward: 2.043. Training.\n",
            "[INFO] AgentBehavior. Step: 3810000. Time Elapsed: 24583.390 s. Mean Reward: 0.546. Std of Reward: 2.079. Training.\n",
            "[INFO] AgentBehavior. Step: 3820000. Time Elapsed: 24649.974 s. Mean Reward: 0.890. Std of Reward: 2.293. Training.\n",
            "[INFO] AgentBehavior. Step: 3830000. Time Elapsed: 24714.266 s. Mean Reward: 0.652. Std of Reward: 2.212. Training.\n",
            "[INFO] AgentBehavior. Step: 3840000. Time Elapsed: 24780.152 s. Mean Reward: 0.613. Std of Reward: 2.281. Training.\n",
            "[INFO] AgentBehavior. Step: 3850000. Time Elapsed: 24842.596 s. Mean Reward: 1.213. Std of Reward: 2.433. Training.\n",
            "[INFO] AgentBehavior. Step: 3860000. Time Elapsed: 24908.809 s. Mean Reward: 0.228. Std of Reward: 2.407. Training.\n",
            "[INFO] AgentBehavior. Step: 3870000. Time Elapsed: 24972.543 s. Mean Reward: 0.867. Std of Reward: 2.270. Training.\n",
            "[INFO] AgentBehavior. Step: 3880000. Time Elapsed: 25037.480 s. Mean Reward: 0.015. Std of Reward: 2.079. Training.\n",
            "[INFO] AgentBehavior. Step: 3890000. Time Elapsed: 25099.899 s. Mean Reward: 0.794. Std of Reward: 2.369. Training.\n",
            "[INFO] AgentBehavior. Step: 3900000. Time Elapsed: 25164.813 s. Mean Reward: 1.051. Std of Reward: 2.442. Training.\n",
            "[INFO] AgentBehavior. Step: 3910000. Time Elapsed: 25227.889 s. Mean Reward: 0.576. Std of Reward: 2.018. Training.\n",
            "[INFO] AgentBehavior. Step: 3920000. Time Elapsed: 25293.146 s. Mean Reward: 1.084. Std of Reward: 2.438. Training.\n",
            "[INFO] AgentBehavior. Step: 3930000. Time Elapsed: 25356.469 s. Mean Reward: 0.404. Std of Reward: 1.960. Training.\n",
            "[INFO] AgentBehavior. Step: 3940000. Time Elapsed: 25422.030 s. Mean Reward: 0.451. Std of Reward: 2.279. Training.\n",
            "[INFO] AgentBehavior. Step: 3950000. Time Elapsed: 25485.094 s. Mean Reward: 0.766. Std of Reward: 2.152. Training.\n",
            "[INFO] AgentBehavior. Step: 3960000. Time Elapsed: 25550.875 s. Mean Reward: 0.609. Std of Reward: 2.070. Training.\n",
            "[INFO] AgentBehavior. Step: 3970000. Time Elapsed: 25617.052 s. Mean Reward: 1.143. Std of Reward: 2.371. Training.\n",
            "[INFO] AgentBehavior. Step: 3980000. Time Elapsed: 25681.572 s. Mean Reward: 0.552. Std of Reward: 2.017. Training.\n",
            "[INFO] AgentBehavior. Step: 3990000. Time Elapsed: 25747.520 s. Mean Reward: 0.735. Std of Reward: 2.236. Training.\n",
            "[INFO] AgentBehavior. Step: 4000000. Time Elapsed: 25810.895 s. Mean Reward: 0.677. Std of Reward: 2.263. Training.\n",
            "[INFO] Exported results/ColabTest/AgentBehavior/AgentBehavior-3999997.onnx\n",
            "[INFO] AgentBehavior. Step: 4010000. Time Elapsed: 25875.979 s. Mean Reward: 0.706. Std of Reward: 2.212. Training.\n",
            "[INFO] AgentBehavior. Step: 4020000. Time Elapsed: 25940.125 s. Mean Reward: 0.456. Std of Reward: 2.038. Training.\n",
            "[INFO] AgentBehavior. Step: 4030000. Time Elapsed: 26004.961 s. Mean Reward: 0.675. Std of Reward: 2.302. Training.\n",
            "[INFO] AgentBehavior. Step: 4040000. Time Elapsed: 26068.234 s. Mean Reward: 0.528. Std of Reward: 2.061. Training.\n",
            "[INFO] AgentBehavior. Step: 4050000. Time Elapsed: 26133.969 s. Mean Reward: 0.506. Std of Reward: 2.143. Training.\n",
            "[INFO] AgentBehavior. Step: 4060000. Time Elapsed: 26197.817 s. Mean Reward: 0.171. Std of Reward: 2.050. Training.\n",
            "[INFO] AgentBehavior. Step: 4070000. Time Elapsed: 26263.171 s. Mean Reward: 0.798. Std of Reward: 2.257. Training.\n",
            "[INFO] AgentBehavior. Step: 4080000. Time Elapsed: 26327.199 s. Mean Reward: 0.586. Std of Reward: 2.001. Training.\n",
            "[INFO] AgentBehavior. Step: 4090000. Time Elapsed: 26393.522 s. Mean Reward: 0.739. Std of Reward: 2.131. Training.\n",
            "[INFO] AgentBehavior. Step: 4100000. Time Elapsed: 26458.355 s. Mean Reward: 1.151. Std of Reward: 2.309. Training.\n",
            "[INFO] AgentBehavior. Step: 4110000. Time Elapsed: 26525.313 s. Mean Reward: 0.973. Std of Reward: 2.310. Training.\n",
            "[INFO] AgentBehavior. Step: 4120000. Time Elapsed: 26590.944 s. Mean Reward: 0.633. Std of Reward: 2.205. Training.\n",
            "[INFO] AgentBehavior. Step: 4130000. Time Elapsed: 26657.632 s. Mean Reward: 0.997. Std of Reward: 2.362. Training.\n",
            "[INFO] AgentBehavior. Step: 4140000. Time Elapsed: 26720.666 s. Mean Reward: 0.610. Std of Reward: 2.080. Training.\n",
            "[INFO] AgentBehavior. Step: 4150000. Time Elapsed: 26788.583 s. Mean Reward: 0.169. Std of Reward: 2.021. Training.\n",
            "[INFO] AgentBehavior. Step: 4160000. Time Elapsed: 26855.901 s. Mean Reward: 0.022. Std of Reward: 1.628. Training.\n",
            "[INFO] AgentBehavior. Step: 4170000. Time Elapsed: 26924.489 s. Mean Reward: 0.577. Std of Reward: 2.161. Training.\n",
            "[INFO] AgentBehavior. Step: 4180000. Time Elapsed: 26990.903 s. Mean Reward: 0.803. Std of Reward: 2.255. Training.\n",
            "[INFO] AgentBehavior. Step: 4190000. Time Elapsed: 27059.360 s. Mean Reward: 0.460. Std of Reward: 2.128. Training.\n",
            "[INFO] AgentBehavior. Step: 4200000. Time Elapsed: 27126.060 s. Mean Reward: 0.721. Std of Reward: 2.201. Training.\n",
            "[INFO] AgentBehavior. Step: 4210000. Time Elapsed: 27194.995 s. Mean Reward: 0.739. Std of Reward: 2.280. Training.\n",
            "[INFO] AgentBehavior. Step: 4220000. Time Elapsed: 27261.270 s. Mean Reward: 0.746. Std of Reward: 2.079. Training.\n",
            "[INFO] AgentBehavior. Step: 4230000. Time Elapsed: 27327.515 s. Mean Reward: 0.564. Std of Reward: 2.069. Training.\n",
            "[INFO] AgentBehavior. Step: 4240000. Time Elapsed: 27394.632 s. Mean Reward: 0.512. Std of Reward: 1.980. Training.\n",
            "[INFO] AgentBehavior. Step: 4250000. Time Elapsed: 27460.738 s. Mean Reward: 1.071. Std of Reward: 2.087. Training.\n",
            "[INFO] AgentBehavior. Step: 4260000. Time Elapsed: 27528.271 s. Mean Reward: 1.139. Std of Reward: 2.417. Training.\n",
            "[INFO] AgentBehavior. Step: 4270000. Time Elapsed: 27596.247 s. Mean Reward: 0.645. Std of Reward: 2.109. Training.\n",
            "[INFO] AgentBehavior. Step: 4280000. Time Elapsed: 27661.248 s. Mean Reward: 1.042. Std of Reward: 2.334. Training.\n",
            "[INFO] AgentBehavior. Step: 4290000. Time Elapsed: 27727.288 s. Mean Reward: 0.788. Std of Reward: 2.212. Training.\n",
            "[INFO] AgentBehavior. Step: 4300000. Time Elapsed: 27791.424 s. Mean Reward: 0.170. Std of Reward: 2.254. Training.\n",
            "[INFO] AgentBehavior. Step: 4310000. Time Elapsed: 27857.712 s. Mean Reward: 0.692. Std of Reward: 2.554. Training.\n",
            "[INFO] AgentBehavior. Step: 4320000. Time Elapsed: 27922.987 s. Mean Reward: 0.614. Std of Reward: 2.164. Training.\n",
            "[INFO] AgentBehavior. Step: 4330000. Time Elapsed: 27987.229 s. Mean Reward: 0.312. Std of Reward: 2.129. Training.\n",
            "[INFO] AgentBehavior. Step: 4340000. Time Elapsed: 28052.356 s. Mean Reward: 0.565. Std of Reward: 2.284. Training.\n",
            "[INFO] AgentBehavior. Step: 4350000. Time Elapsed: 28115.527 s. Mean Reward: 0.611. Std of Reward: 2.104. Training.\n",
            "[INFO] AgentBehavior. Step: 4360000. Time Elapsed: 28180.653 s. Mean Reward: 0.476. Std of Reward: 1.947. Training.\n",
            "[INFO] AgentBehavior. Step: 4370000. Time Elapsed: 28243.533 s. Mean Reward: 0.679. Std of Reward: 2.166. Training.\n",
            "[INFO] AgentBehavior. Step: 4380000. Time Elapsed: 28308.826 s. Mean Reward: 0.250. Std of Reward: 2.015. Training.\n",
            "[INFO] AgentBehavior. Step: 4390000. Time Elapsed: 28373.169 s. Mean Reward: 0.499. Std of Reward: 2.144. Training.\n",
            "[INFO] AgentBehavior. Step: 4400000. Time Elapsed: 28438.226 s. Mean Reward: 0.600. Std of Reward: 2.246. Training.\n",
            "[INFO] AgentBehavior. Step: 4410000. Time Elapsed: 28502.952 s. Mean Reward: 0.632. Std of Reward: 2.157. Training.\n",
            "[INFO] AgentBehavior. Step: 4420000. Time Elapsed: 28568.728 s. Mean Reward: 0.334. Std of Reward: 1.985. Training.\n",
            "[INFO] AgentBehavior. Step: 4430000. Time Elapsed: 28633.405 s. Mean Reward: 1.032. Std of Reward: 2.321. Training.\n",
            "[INFO] AgentBehavior. Step: 4440000. Time Elapsed: 28699.601 s. Mean Reward: 0.055. Std of Reward: 1.969. Training.\n",
            "[INFO] AgentBehavior. Step: 4450000. Time Elapsed: 28763.161 s. Mean Reward: 0.451. Std of Reward: 2.253. Training.\n",
            "[INFO] AgentBehavior. Step: 4460000. Time Elapsed: 28831.085 s. Mean Reward: 1.014. Std of Reward: 2.129. Training.\n",
            "[INFO] AgentBehavior. Step: 4470000. Time Elapsed: 28894.462 s. Mean Reward: -0.194. Std of Reward: 1.814. Training.\n",
            "[INFO] AgentBehavior. Step: 4480000. Time Elapsed: 28960.658 s. Mean Reward: 0.389. Std of Reward: 2.134. Training.\n",
            "[INFO] Learning was interrupted. Please wait while the graph is generated.\n",
            "[INFO] Exported results/ColabTest/AgentBehavior/AgentBehavior-4483526.onnx\n",
            "[INFO] Copied results/ColabTest/AgentBehavior/AgentBehavior-4483526.onnx to results/ColabTest/AgentBehavior.onnx.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/files.zip /content/results"
      ],
      "metadata": {
        "id": "7MtTYg2c95XR",
        "outputId": "45f01555-b287-4ae6-a6e8-0efb52241213",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/results/ (stored 0%)\n",
            "  adding: content/results/ColabTest/ (stored 0%)\n",
            "  adding: content/results/ColabTest/run_logs/ (stored 0%)\n",
            "  adding: content/results/ColabTest/run_logs/training_status.json (deflated 82%)\n",
            "  adding: content/results/ColabTest/run_logs/timers.json (deflated 86%)\n",
            "  adding: content/results/ColabTest/run_logs/Player-3.log (deflated 77%)\n",
            "  adding: content/results/ColabTest/run_logs/Player-2.log (deflated 77%)\n",
            "  adding: content/results/ColabTest/run_logs/Player-0.log (deflated 77%)\n",
            "  adding: content/results/ColabTest/run_logs/Player-1.log (deflated 76%)\n",
            "  adding: content/results/ColabTest/AgentBehavior/ (stored 0%)\n",
            "  adding: content/results/ColabTest/AgentBehavior/AgentBehavior-3499998.onnx (deflated 8%)\n",
            "  adding: content/results/ColabTest/AgentBehavior/AgentBehavior-2999942.onnx (deflated 8%)\n",
            "  adding: content/results/ColabTest/AgentBehavior/AgentBehavior-2499997.onnx (deflated 8%)\n",
            "  adding: content/results/ColabTest/AgentBehavior/AgentBehavior-3499998.pt (deflated 6%)\n",
            "  adding: content/results/ColabTest/AgentBehavior/AgentBehavior-2499997.pt (deflated 6%)\n",
            "  adding: content/results/ColabTest/AgentBehavior/AgentBehavior-4483526.pt (deflated 6%)\n",
            "  adding: content/results/ColabTest/AgentBehavior/AgentBehavior-3999997.onnx (deflated 8%)\n",
            "  adding: content/results/ColabTest/AgentBehavior/AgentBehavior-2999942.pt (deflated 6%)\n",
            "  adding: content/results/ColabTest/AgentBehavior/AgentBehavior-4483526.onnx (deflated 8%)\n",
            "  adding: content/results/ColabTest/AgentBehavior/AgentBehavior-3999997.pt (deflated 6%)\n",
            "  adding: content/results/ColabTest/AgentBehavior/checkpoint.pt (deflated 6%)\n",
            "  adding: content/results/ColabTest/AgentBehavior/events.out.tfevents.1710541824.671fed95192d.5577.0 (deflated 96%)\n",
            "  adding: content/results/ColabTest/AgentBehavior.onnx (deflated 8%)\n",
            "  adding: content/results/ColabTest/configuration.yaml (deflated 59%)\n"
          ]
        }
      ]
    }
  ]
}